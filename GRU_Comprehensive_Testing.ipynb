{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive GRU Architecture Testing for VIX Forecasting\n",
    "\n",
    "This notebook implements comprehensive GRU architecture testing with:\n",
    "- **Multiple Architecture Variants**: Basic, Deep, Bidirectional, Attention, Residual, Dropout-Enhanced\n",
    "- **Hyperparameter Optimization**: Optuna-based optimization for each architecture\n",
    "- **Time Series Cross-Validation**: Proper temporal validation\n",
    "- **Statistical Significance Testing**: Robust model comparison\n",
    "- **Comprehensive Training & Evaluation**: Full pipeline for each variant\n",
    "- **Results Storage**: Systematic storage of all results for comparison\n",
    "\n",
    "## Architecture Variants Tested:\n",
    "1. **Basic GRU**: Baseline architecture with standard GRU layers\n",
    "2. **Deep GRU**: Enhanced depth with multiple GRU layers\n",
    "3. **Bidirectional GRU**: Bidirectional GRU for better temporal modeling\n",
    "4. **Attention GRU**: Multi-head attention mechanism for feature importance\n",
    "5. **Residual GRU**: Residual connections for better gradient flow\n",
    "6. **Dropout-Enhanced GRU**: Advanced regularization with multiple dropout strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shared utilities\n",
    "from vix_research_utils import *\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, GRU, Input, MultiHeadAttention, LayerNormalization,\n",
    "    Bidirectional, BatchNormalization, GlobalAveragePooling1D, Add,\n",
    "    GaussianNoise, Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Additional imports\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure TensorFlow\n",
    "tf.config.experimental.enable_memory_growth = True\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 2: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data using shared utilities\n",
    "print(\"Loading VIX and VVIX data...\")\n",
    "vix_data, vvix_data = download_market_data()\n",
    "\n",
    "print(\"\\nCleaning and preprocessing data...\")\n",
    "vix_clean = clean_data(vix_data)\n",
    "vvix_clean = clean_data(vvix_data)\n",
    "\n",
    "print(\"\\nCreating features...\")\n",
    "features_df = create_features(vix_clean, vvix_clean)\n",
    "\n",
    "print(\"\\nPreparing sequences for deep learning...\")\n",
    "X, y, feature_names, scaler = prepare_sequences(features_df, sequence_length=30)\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "\n",
    "# Split data for time series\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 3: GRU Architecture Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_basic_gru(trial, input_shape):\n",
    "    \"\"\"Basic GRU architecture with hyperparameter optimization\"\"\"\n",
    "    # Hyperparameters\n",
    "    gru_units_1 = trial.suggest_int('gru_units_1', 32, 128, step=32)\n",
    "    gru_units_2 = trial.suggest_int('gru_units_2', 16, 64, step=16)\n",
    "    dense_units = trial.suggest_int('dense_units', 8, 32, step=8)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        GRU(gru_units_1, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        GRU(gru_units_2),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_deep_gru(trial, input_shape):\n",
    "    \"\"\"Deep GRU with multiple layers\"\"\"\n",
    "    # Hyperparameters\n",
    "    gru_units_1 = trial.suggest_int('gru_units_1', 64, 256, step=64)\n",
    "    gru_units_2 = trial.suggest_int('gru_units_2', 32, 128, step=32)\n",
    "    gru_units_3 = trial.suggest_int('gru_units_3', 16, 64, step=16)\n",
    "    dense_units_1 = trial.suggest_int('dense_units_1', 16, 64, step=16)\n",
    "    dense_units_2 = trial.suggest_int('dense_units_2', 8, 32, step=8)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Deep GRU layers\n",
    "        GRU(gru_units_1, return_sequences=True, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        GRU(gru_units_2, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        GRU(gru_units_3),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(dense_units_1, activation='relu'),\n",
    "        Dropout(dropout_rate * 0.7),\n",
    "        Dense(dense_units_2, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_bidirectional_gru(trial, input_shape):\n",
    "    \"\"\"GRU with bidirectional layers\"\"\"\n",
    "    # Hyperparameters\n",
    "    gru_units_1 = trial.suggest_int('gru_units_1', 32, 128, step=32)\n",
    "    gru_units_2 = trial.suggest_int('gru_units_2', 16, 64, step=16)\n",
    "    dense_units = trial.suggest_int('dense_units', 16, 64, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Bidirectional GRU layers\n",
    "        Bidirectional(GRU(gru_units_1, return_sequences=True), input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        Bidirectional(GRU(gru_units_2)),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate * 0.7),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_attention_gru(trial, input_shape):\n",
    "    \"\"\"GRU with multi-head attention mechanism\"\"\"\n",
    "    # Hyperparameters\n",
    "    gru_units = trial.suggest_int('gru_units', 32, 128, step=32)\n",
    "    attention_heads = trial.suggest_int('attention_heads', 2, 8, step=2)\n",
    "    dense_units = trial.suggest_int('dense_units', 16, 64, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # GRU layer\n",
    "    gru_out = GRU(gru_units, return_sequences=True)(inputs)\n",
    "    gru_out = Dropout(dropout_rate)(gru_out)\n",
    "    \n",
    "    # Multi-head attention\n",
    "    attention_out = MultiHeadAttention(\n",
    "        num_heads=attention_heads, \n",
    "        key_dim=gru_units // attention_heads\n",
    "    )(gru_out, gru_out)\n",
    "    \n",
    "    # Layer normalization and residual connection\n",
    "    attention_out = LayerNormalization()(attention_out)\n",
    "    combined = Add()([gru_out, attention_out])\n",
    "    \n",
    "    # Global pooling\n",
    "    pooled = GlobalAveragePooling1D()(combined)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(dense_units, activation='relu')(pooled)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_residual_gru(trial, input_shape):\n",
    "    \"\"\"GRU with residual connections\"\"\"\n",
    "    # Hyperparameters\n",
    "    gru_units_1 = trial.suggest_int('gru_units_1', 32, 128, step=32)\n",
    "    gru_units_2 = trial.suggest_int('gru_units_2', 32, 128, step=32)\n",
    "    dense_units = trial.suggest_int('dense_units', 16, 64, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # First GRU block\n",
    "    gru1 = GRU(gru_units_1, return_sequences=True)(inputs)\n",
    "    gru1 = Dropout(dropout_rate)(gru1)\n",
    "    \n",
    "    # Second GRU block with residual connection\n",
    "    gru2 = GRU(gru_units_2, return_sequences=True)(gru1)\n",
    "    gru2 = Dropout(dropout_rate)(gru2)\n",
    "    \n",
    "    # Residual connection (if dimensions match)\n",
    "    if gru_units_1 == gru_units_2:\n",
    "        combined = Add()([gru1, gru2])\n",
    "    else:\n",
    "        combined = gru2\n",
    "    \n",
    "    # Final GRU layer\n",
    "    final_gru = GRU(gru_units_2 // 2)(combined)\n",
    "    final_gru = Dropout(dropout_rate)(final_gru)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(dense_units, activation='relu')(final_gru)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 4: Advanced GRU Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dropout_enhanced_gru(trial, input_shape):\n",
    "    \"\"\"GRU with enhanced dropout regularization\"\"\"\n",
    "    # Hyperparameters\n",
    "    gru_units_1 = trial.suggest_int('gru_units_1', 32, 128, step=32)\n",
    "    gru_units_2 = trial.suggest_int('gru_units_2', 16, 64, step=16)\n",
    "    gru_units_3 = trial.suggest_int('gru_units_3', 8, 32, step=8)\n",
    "    dense_units_1 = trial.suggest_int('dense_units_1', 16, 64, step=16)\n",
    "    dense_units_2 = trial.suggest_int('dense_units_2', 8, 32, step=8)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    recurrent_dropout = trial.suggest_float('recurrent_dropout', 0.1, 0.4)\n",
    "    noise_level = trial.suggest_float('noise_level', 0.05, 0.2)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Input noise for regularization\n",
    "        GaussianNoise(noise_level, input_shape=input_shape),\n",
    "        \n",
    "        # Enhanced dropout GRU layers\n",
    "        GRU(gru_units_1, return_sequences=True, \n",
    "            dropout=dropout_rate, recurrent_dropout=recurrent_dropout),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate * 1.2),\n",
    "        \n",
    "        GRU(gru_units_2, return_sequences=True,\n",
    "            dropout=dropout_rate, recurrent_dropout=recurrent_dropout),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate * 1.2),\n",
    "        \n",
    "        GRU(gru_units_3, dropout=dropout_rate, recurrent_dropout=recurrent_dropout),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Dense layers with progressive dropout\n",
    "        Dense(dense_units_1, activation='relu'),\n",
    "        Dropout(dropout_rate * 0.8),\n",
    "        Dense(dense_units_2, activation='relu'),\n",
    "        Dropout(dropout_rate * 0.6),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Architecture registry\n",
    "GRU_ARCHITECTURES = {\n",
    "    'Basic_GRU': build_basic_gru,\n",
    "    'Deep_GRU': build_deep_gru,\n",
    "    'Bidirectional_GRU': build_bidirectional_gru,\n",
    "    'Attention_GRU': build_attention_gru,\n",
    "    'Residual_GRU': build_residual_gru,\n",
    "    'Dropout_Enhanced_GRU': build_dropout_enhanced_gru\n",
    "}\n",
    "\n",
    "print(f\"GRU architectures available: {list(GRU_ARCHITECTURES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 5: Hyperparameter Optimization Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(trial, architecture_name, X_train, y_train, input_shape):\n",
    "    \"\"\"Objective function for hyperparameter optimization\"\"\"\n",
    "    try:\n",
    "        # Build model with trial hyperparameters\n",
    "        model_builder = GRU_ARCHITECTURES[architecture_name]\n",
    "        model = model_builder(trial, input_shape)\n",
    "        \n",
    "        # Time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in tscv.split(X_train):\n",
    "            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "            \n",
    "            # Callbacks\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss', patience=10, restore_best_weights=True\n",
    "            )\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6\n",
    "            )\n",
    "            pruning_callback = TFKerasPruningCallback(trial, 'val_loss')\n",
    "            \n",
    "            # Train model\n",
    "            history = model.fit(\n",
    "                X_fold_train, y_fold_train,\n",
    "                validation_data=(X_fold_val, y_fold_val),\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stopping, reduce_lr, pruning_callback],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Get validation score\n",
    "            val_loss = min(history.history['val_loss'])\n",
    "            cv_scores.append(val_loss)\n",
    "            \n",
    "            # Clear memory\n",
    "            tf.keras.backend.clear_session()\n",
    "        \n",
    "        # Return mean CV score\n",
    "        return np.mean(cv_scores)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "def optimize_architecture(architecture_name, X_train, y_train, input_shape, n_trials=100):\n",
    "    \"\"\"Optimize hyperparameters for a specific architecture\"\"\"\n",
    "    print(f\"\\nOptimizing {architecture_name}...\")\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        study_name=f'{architecture_name}_optimization',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42)\n",
    "    )\n",
    "    \n",
    "    # Optimize\n",
    "    study.optimize(\n",
    "        lambda trial: objective_function(trial, architecture_name, X_train, y_train, input_shape),\n",
    "        n_trials=n_trials,\n",
    "        timeout=3600  # 1 hour timeout\n",
    "    )\n",
    "    \n",
    "    print(f\"Best trial for {architecture_name}:\")\n",
    "    print(f\"  Value: {study.best_trial.value:.6f}\")\n",
    "    print(f\"  Params: {study.best_trial.params}\")\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 6: Training and Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(architecture_name, best_params, X_train, y_train, X_test, y_test, input_shape):\n",
    "    \"\"\"Train final model with best hyperparameters\"\"\"\n",
    "    print(f\"\\nTraining final {architecture_name} model...\")\n",
    "    \n",
    "    # Create mock trial with best parameters\n",
    "    class MockTrial:\n",
    "        def __init__(self, params):\n",
    "            self.params = params\n",
    "        \n",
    "        def suggest_int(self, name, low, high, step=1):\n",
    "            return self.params.get(name, low)\n",
    "        \n",
    "        def suggest_float(self, name, low, high, log=False):\n",
    "            return self.params.get(name, low)\n",
    "    \n",
    "    mock_trial = MockTrial(best_params)\n",
    "    \n",
    "    # Build model with best parameters\n",
    "    model_builder = GRU_ARCHITECTURES[architecture_name]\n",
    "    model = model_builder(mock_trial, input_shape)\n",
    "    \n",
    "    # Callbacks for final training\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=20, restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Train final model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = model.predict(X_train, verbose=0)\n",
    "    test_pred = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_metrics = calculate_metrics(y_train, train_pred.flatten())\n",
    "    test_metrics = calculate_metrics(y_test, test_pred.flatten())\n",
    "    \n",
    "    results = {\n",
    "        'architecture': architecture_name,\n",
    "        'best_params': best_params,\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'train_predictions': train_pred.flatten(),\n",
    "        'test_predictions': test_pred.flatten()\n",
    "    }\n",
    "    \n",
    "    print(f\"Final {architecture_name} Results:\")\n",
    "    print(f\"  Train MSE: {train_metrics['mse']:.6f}\")\n",
    "    print(f\"  Test MSE: {test_metrics['mse']:.6f}\")\n",
    "    print(f\"  Test MAE: {test_metrics['mae']:.6f}\")\n",
    "    print(f\"  Test R²: {test_metrics['r2']:.6f}\")\n",
    "    \n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
