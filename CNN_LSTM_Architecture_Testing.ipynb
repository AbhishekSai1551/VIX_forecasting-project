{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM Architecture Testing for VIX Forecasting\n",
    "\n",
    "This notebook implements comprehensive CNN-LSTM architecture testing with:\n",
    "\n",
    "- **Multiple Architecture Variants**: Basic, Deep, Bidirectional, Attention, Multiscale\n",
    "- **Hyperparameter Optimization**: Optuna-based optimization for each architecture\n",
    "- **Time Series Cross-Validation**: Proper temporal validation\n",
    "- **Statistical Significance Testing**: Robust model comparison\n",
    "- **Comprehensive Training & Evaluation**: Full pipeline for each variant\n",
    "- **Results Storage**: Systematic storage of all results for comparison\n",
    "\n",
    "## Architecture Variants Tested:\n",
    "\n",
    "1. **Basic CNN-LSTM**: Baseline architecture with standard CNN + LSTM layers\n",
    "2. **Deep CNN-LSTM**: Enhanced depth with multiple CNN and LSTM layers\n",
    "3. **Bidirectional CNN-LSTM**: Bidirectional LSTM for better temporal modeling\n",
    "4. **Attention CNN-LSTM**: Multi-head attention mechanism for feature importance\n",
    "5. **Multiscale CNN-LSTM**: Multiple kernel sizes for multi-scale feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shared utilities\n",
    "from vix_research_utils import *\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Input, \n",
    "    MultiHeadAttention, LayerNormalization, Bidirectional, \n",
    "    BatchNormalization, GlobalAveragePooling1D, GlobalMaxPooling1D,\n",
    "    Concatenate, Add, GaussianNoise, Flatten\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Additional imports\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure TensorFlow\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 2: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data using shared utilities\n",
    "print(\"Loading VIX and VVIX data...\")\n",
    "vix_data, vvix_data = download_market_data()\n",
    "\n",
    "print(\"\\nCreating features...\")\n",
    "features_df = create_features(vix_data, vvix_data)\n",
    "\n",
    "print(\"\\nPreparing sequences for deep learning...\")\n",
    "X, y, feature_names, scaler = prepare_sequences(features_df, sequence_length=30)\n",
    "\n",
    "print(f\"\\nData shapes:\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "\n",
    "# Split data for time series\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "print(f\"Input shape: {input_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 3: CNN-LSTM Architecture Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_basic_cnn_lstm(trial, input_shape):\n",
    "    \"\"\"Basic CNN-LSTM architecture with hyperparameter optimization\"\"\"\n",
    "    # Hyperparameters\n",
    "    cnn_filters_1 = trial.suggest_int('cnn_filters_1', 32, 128, step=32)\n",
    "    cnn_filters_2 = trial.suggest_int('cnn_filters_2', 16, 64, step=16)\n",
    "    lstm_units_1 = trial.suggest_int('lstm_units_1', 32, 128, step=32)\n",
    "    lstm_units_2 = trial.suggest_int('lstm_units_2', 16, 64, step=16)\n",
    "    dense_units = trial.suggest_int('dense_units', 8, 32, step=8)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        # CNN layers\n",
    "        Conv1D(filters=cnn_filters_1, kernel_size=3, activation='relu', \n",
    "               padding='same', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=cnn_filters_2, kernel_size=3, activation='relu', padding='same'),\n",
    "        \n",
    "        # LSTM layers\n",
    "        LSTM(lstm_units_1, return_sequences=True),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units_2),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_deep_cnn_lstm(trial, input_shape):\n",
    "    \"\"\"Deep CNN-LSTM with multiple layers\"\"\"\n",
    "    # Hyperparameters\n",
    "    cnn_filters_1 = trial.suggest_int('cnn_filters_1', 64, 256, step=64)\n",
    "    cnn_filters_2 = trial.suggest_int('cnn_filters_2', 32, 128, step=32)\n",
    "    cnn_filters_3 = trial.suggest_int('cnn_filters_3', 16, 64, step=16)\n",
    "    lstm_units_1 = trial.suggest_int('lstm_units_1', 64, 256, step=64)\n",
    "    lstm_units_2 = trial.suggest_int('lstm_units_2', 32, 128, step=32)\n",
    "    lstm_units_3 = trial.suggest_int('lstm_units_3', 16, 64, step=16)\n",
    "    dense_units_1 = trial.suggest_int('dense_units_1', 16, 64, step=16)\n",
    "    dense_units_2 = trial.suggest_int('dense_units_2', 8, 32, step=8)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Deep CNN layers\n",
    "        Conv1D(filters=cnn_filters_1, kernel_size=3, activation='relu', \n",
    "               padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        Conv1D(filters=cnn_filters_2, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        Conv1D(filters=cnn_filters_3, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Deep LSTM layers\n",
    "        LSTM(lstm_units_1, return_sequences=True),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units_2, return_sequences=True),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units_3),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(dense_units_1, activation='relu'),\n",
    "        Dropout(dropout_rate * 0.7),\n",
    "        Dense(dense_units_2, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_bidirectional_cnn_lstm(trial, input_shape):\n",
    "    \"\"\"CNN-LSTM with bidirectional LSTM layers\"\"\"\n",
    "    # Hyperparameters\n",
    "    cnn_filters_1 = trial.suggest_int('cnn_filters_1', 32, 128, step=32)\n",
    "    cnn_filters_2 = trial.suggest_int('cnn_filters_2', 16, 64, step=16)\n",
    "    lstm_units_1 = trial.suggest_int('lstm_units_1', 32, 128, step=32)\n",
    "    lstm_units_2 = trial.suggest_int('lstm_units_2', 16, 64, step=16)\n",
    "    dense_units = trial.suggest_int('dense_units', 16, 64, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        # CNN layers\n",
    "        Conv1D(filters=cnn_filters_1, kernel_size=3, activation='relu', \n",
    "               padding='same', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=cnn_filters_2, kernel_size=3, activation='relu', padding='same'),\n",
    "        \n",
    "        # Bidirectional LSTM layers\n",
    "        Bidirectional(LSTM(lstm_units_1, return_sequences=True)),\n",
    "        Dropout(dropout_rate),\n",
    "        Bidirectional(LSTM(lstm_units_2)),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate * 0.7),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 4: Advanced CNN-LSTM Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_attention_cnn_lstm(trial, input_shape):\n",
    "    \"\"\"CNN-LSTM with multi-head attention mechanism\"\"\"\n",
    "    # Hyperparameters\n",
    "    cnn_filters_1 = trial.suggest_int('cnn_filters_1', 32, 128, step=32)\n",
    "    cnn_filters_2 = trial.suggest_int('cnn_filters_2', 16, 64, step=16)\n",
    "    lstm_units = trial.suggest_int('lstm_units', 32, 128, step=32)\n",
    "    attention_heads = trial.suggest_int('attention_heads', 2, 8, step=2)\n",
    "    dense_units = trial.suggest_int('dense_units', 16, 64, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # CNN layers\n",
    "    x = Conv1D(filters=cnn_filters_1, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(filters=cnn_filters_2, kernel_size=3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # LSTM layer\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(x)\n",
    "    lstm_out = Dropout(dropout_rate)(lstm_out)\n",
    "    \n",
    "    # Multi-head attention\n",
    "    attention_out = MultiHeadAttention(\n",
    "        num_heads=attention_heads, \n",
    "        key_dim=lstm_units // attention_heads\n",
    "    )(lstm_out, lstm_out)\n",
    "    \n",
    "    # Layer normalization and residual connection\n",
    "    attention_out = LayerNormalization()(attention_out)\n",
    "    combined = Add()([lstm_out, attention_out])\n",
    "    \n",
    "    # Global pooling\n",
    "    pooled = GlobalAveragePooling1D()(combined)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(dense_units, activation='relu')(pooled)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_multiscale_cnn_lstm(trial, input_shape):\n",
    "    \"\"\"CNN-LSTM with multiple kernel sizes for multi-scale feature extraction\"\"\"\n",
    "    # Hyperparameters\n",
    "    filters_small = trial.suggest_int('filters_small', 16, 64, step=16)\n",
    "    filters_medium = trial.suggest_int('filters_medium', 16, 64, step=16)\n",
    "    filters_large = trial.suggest_int('filters_large', 16, 64, step=16)\n",
    "    lstm_units = trial.suggest_int('lstm_units', 32, 128, step=32)\n",
    "    dense_units = trial.suggest_int('dense_units', 16, 64, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Multi-scale CNN branches\n",
    "    # Small kernel (short-term patterns)\n",
    "    conv_small = Conv1D(filters=filters_small, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    conv_small = MaxPooling1D(pool_size=2)(conv_small)\n",
    "    \n",
    "    # Medium kernel (medium-term patterns)\n",
    "    conv_medium = Conv1D(filters=filters_medium, kernel_size=5, activation='relu', padding='same')(inputs)\n",
    "    conv_medium = MaxPooling1D(pool_size=2)(conv_medium)\n",
    "    \n",
    "    # Large kernel (long-term patterns)\n",
    "    conv_large = Conv1D(filters=filters_large, kernel_size=7, activation='relu', padding='same')(inputs)\n",
    "    conv_large = MaxPooling1D(pool_size=2)(conv_large)\n",
    "    \n",
    "    # Concatenate multi-scale features\n",
    "    combined_conv = Concatenate()([conv_small, conv_medium, conv_large])\n",
    "    \n",
    "    # LSTM layers\n",
    "    lstm_out = LSTM(lstm_units, return_sequences=True)(combined_conv)\n",
    "    lstm_out = Dropout(dropout_rate)(lstm_out)\n",
    "    lstm_out = LSTM(lstm_units // 2)(lstm_out)\n",
    "    lstm_out = Dropout(dropout_rate)(lstm_out)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(dense_units, activation='relu')(lstm_out)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Architecture registry\n",
    "CNN_LSTM_ARCHITECTURES = {\n",
    "    'Basic_CNN_LSTM': build_basic_cnn_lstm,\n",
    "    'Deep_CNN_LSTM': build_deep_cnn_lstm,\n",
    "    'Bidirectional_CNN_LSTM': build_bidirectional_cnn_lstm,\n",
    "    'Attention_CNN_LSTM': build_attention_cnn_lstm,\n",
    "    'Multiscale_CNN_LSTM': build_multiscale_cnn_lstm\n",
    "}\n",
    "\n",
    "print(f\"CNN-LSTM architectures available: {list(CNN_LSTM_ARCHITECTURES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 5: Hyperparameter Optimization Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(trial, architecture_name, X_train, y_train, input_shape):\n",
    "    \"\"\"Objective function for hyperparameter optimization\"\"\"\n",
    "    try:\n",
    "        # Build model with trial hyperparameters\n",
    "        model_builder = CNN_LSTM_ARCHITECTURES[architecture_name]\n",
    "        model = model_builder(trial, input_shape)\n",
    "        \n",
    "        # Time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in tscv.split(X_train):\n",
    "            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "            \n",
    "            # Callbacks\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor='val_loss', patience=10, restore_best_weights=True\n",
    "            )\n",
    "            reduce_lr = ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6\n",
    "            )\n",
    "            pruning_callback = TFKerasPruningCallback(trial, 'val_loss')\n",
    "            \n",
    "            # Train model\n",
    "            history = model.fit(\n",
    "                X_fold_train, y_fold_train,\n",
    "                validation_data=(X_fold_val, y_fold_val),\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stopping, reduce_lr, pruning_callback],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Get validation score\n",
    "            val_loss = min(history.history['val_loss'])\n",
    "            cv_scores.append(val_loss)\n",
    "            \n",
    "            # Clear memory\n",
    "            tf.keras.backend.clear_session()\n",
    "        \n",
    "        # Return mean CV score\n",
    "        return np.mean(cv_scores)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "def optimize_architecture(architecture_name, X_train, y_train, input_shape, n_trials=100):\n",
    "    \"\"\"Optimize hyperparameters for a specific architecture\"\"\"\n",
    "    print(f\"\\nOptimizing {architecture_name}...\")\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        study_name=f'{architecture_name}_optimization',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42)\n",
    "    )\n",
    "    \n",
    "    # Optimize\n",
    "    study.optimize(\n",
    "        lambda trial: objective_function(trial, architecture_name, X_train, y_train, input_shape),\n",
    "        n_trials=n_trials,\n",
    "        timeout=3600  # 1 hour timeout\n",
    "    )\n",
    "    \n",
    "    print(f\"Best trial for {architecture_name}:\")\n",
    "    print(f\"  Value: {study.best_trial.value:.6f}\")\n",
    "    print(f\"  Params: {study.best_trial.params}\")\n",
    "    \n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 6: Training and Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(architecture_name, best_params, X_train, y_train, X_test, y_test, input_shape):\n",
    "    \"\"\"Train final model with best hyperparameters\"\"\"\n",
    "    print(f\"\\nTraining final {architecture_name} model...\")\n",
    "    \n",
    "    # Create mock trial with best parameters\n",
    "    class MockTrial:\n",
    "        def __init__(self, params):\n",
    "            self.params = params\n",
    "        \n",
    "        def suggest_int(self, name, low, high, step=1):\n",
    "            return self.params.get(name, low)\n",
    "        \n",
    "        def suggest_float(self, name, low, high, log=False):\n",
    "            return self.params.get(name, low)\n",
    "    \n",
    "    mock_trial = MockTrial(best_params)\n",
    "    \n",
    "    # Build model with best parameters\n",
    "    model_builder = CNN_LSTM_ARCHITECTURES[architecture_name]\n",
    "    model = model_builder(mock_trial, input_shape)\n",
    "    \n",
    "    # Callbacks for final training\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss', patience=20, restore_best_weights=True\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Train final model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    train_pred = model.predict(X_train, verbose=0)\n",
    "    test_pred = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_metrics = calculate_metrics(y_train, train_pred.flatten())\n",
    "    test_metrics = calculate_metrics(y_test, test_pred.flatten())\n",
    "    \n",
    "    results = {\n",
    "        'architecture': architecture_name,\n",
    "        'best_params': best_params,\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'train_predictions': train_pred.flatten(),\n",
    "        'test_predictions': test_pred.flatten()\n",
    "    }\n",
    "    \n",
    "    print(f\"Final {architecture_name} Results:\")\n",
    "    print(f\"  Train MSE: {train_metrics['MSE']:.6f}\")\n",
    "    print(f\"  Test MSE: {test_metrics['MSE']:.6f}\")\n",
    "    print(f\"  Test MAE: {test_metrics['MAE']:.6f}\")\n",
    "    print(f\"  Test R²: {test_metrics['R2']:.6f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 7: Execute Comprehensive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute comprehensive testing for all CNN-LSTM architectures\n",
    "print(\"Starting CNN-LSTM Architecture Testing...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_results = {}\n",
    "all_studies = {}\n",
    "\n",
    "for architecture_name in CNN_LSTM_ARCHITECTURES.keys():\n",
    "    print(f\"\\nTesting {architecture_name}...\")\n",
    "    \n",
    "    # Optimize hyperparameters\n",
    "    study = optimize_architecture(architecture_name, X_train, y_train, input_shape, n_trials=50)\n",
    "    all_studies[architecture_name] = study\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    best_params = study.best_trial.params\n",
    "    results = train_final_model(architecture_name, best_params, X_train, y_train, X_test, y_test, input_shape)\n",
    "    all_results[architecture_name] = results\n",
    "    \n",
    "    # Save individual results\n",
    "    save_model_results(results, f'cnn_lstm_results_{architecture_name.lower()}.pkl')\n",
    "    \n",
    "    print(f\"Completed {architecture_name}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Save comprehensive results\n",
    "comprehensive_results = {\n",
    "    'results': all_results,\n",
    "    'studies': all_studies,\n",
    "    'data_info': {\n",
    "        'input_shape': input_shape,\n",
    "        'train_samples': X_train.shape[0],\n",
    "        'test_samples': X_test.shape[0],\n",
    "        'features': feature_names\n",
    "    }\n",
    "}\n",
    "\n",
    "save_model_results(comprehensive_results, 'cnn_lstm_comprehensive_results.pkl')\n",
    "\n",
    "print(\"\\nCNN-LSTM Architecture Testing Complete!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 8: Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and compare all CNN-LSTM results\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create performance comparison\n",
    "performance_data = []\n",
    "for arch_name, results in all_results.items():\n",
    "    test_metrics = results['test_metrics']\n",
    "    performance_data.append({\n",
    "        'Architecture': arch_name,\n",
    "        'MSE': test_metrics['MSE'],\n",
    "        'MAE': test_metrics['MAE'],\n",
    "        'R2': test_metrics['R2'],\n",
    "        'Directional_Accuracy': test_metrics['Directional_Accuracy']\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "performance_df = performance_df.sort_values('MSE')\n",
    "\n",
    "print(\"CNN-LSTM Architecture Performance Comparison:\")\n",
    "print(\"=\" * 60)\n",
    "print(performance_df.to_string(index=False, float_format='%.6f'))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# MSE Comparison\n",
    "sns.barplot(data=performance_df, x='MSE', y='Architecture', ax=axes[0,0])\n",
    "axes[0,0].set_title('Test MSE Comparison')\n",
    "\n",
    "# R² Comparison\n",
    "sns.barplot(data=performance_df, x='R2', y='Architecture', ax=axes[0,1])\n",
    "axes[0,1].set_title('Test R² Comparison')\n",
    "\n",
    "# MAE Comparison\n",
    "sns.barplot(data=performance_df, x='MAE', y='Architecture', ax=axes[1,0])\n",
    "axes[1,0].set_title('Test MAE Comparison')\n",
    "\n",
    "# Directional Accuracy Comparison\n",
    "sns.barplot(data=performance_df, x='Directional_Accuracy', y='Architecture', ax=axes[1,1])\n",
    "axes[1,1].set_title('Directional Accuracy Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best model identification\n",
    "best_model = performance_df.iloc[0]\n",
    "print(f\"\\nBest CNN-LSTM Architecture: {best_model['Architecture']}\")\n",
    "print(f\"Test MSE: {best_model['MSE']:.6f}\")\n",
    "print(f\"Test R²: {best_model['R2']:.6f}\")\n",
    "print(f\"Directional Accuracy: {best_model['Directional_Accuracy']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}