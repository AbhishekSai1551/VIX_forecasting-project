{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced VIX Forecasting with Market Sentiment and Advanced ML\n",
    "## Comprehensive Analysis with Data Augmentation, Cross-Validation, and Hyperparameter Optimization\n",
    "\n",
    "This notebook implements state-of-the-art VIX forecasting using:\n",
    "1. **Market Sentiment Data**: 10+ sentiment indicators from multiple asset classes\n",
    "2. **Enhanced Feature Engineering**: 50+ technical and sentiment features\n",
    "3. **Data Augmentation**: Noise injection, time warping, regime balancing\n",
    "4. **Cross-Validation**: Time series CV for robust evaluation\n",
    "5. **Hyperparameter Optimization**: Automated tuning with Optuna\n",
    "6. **Advanced Models**: Optimized CNN-LSTM and GRU with attention mechanisms\n",
    "\n",
   "**Expected Improvements**: 15-25% better performance vs baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy.stats import zscore\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# GARCH modeling\n",
    "from arch import arch_model\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, LSTM, GRU, Conv1D, MaxPooling1D, \n",
    "    Input, Attention, MultiHeadAttention, LayerNormalization,\n",
    "    Bidirectional, BatchNormalization, GaussianNoise,\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate,\n",
    "    Multiply, Add\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.callbacks import (\n",
    "    LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, \n",
    "    ModelCheckpoint, TensorBoard\n",
    ")\n",
    "import math\n",
    "\n",
    "# Hyperparameter optimization\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.integration import TFKerasPruningCallback\n",
    "    OPTUNA_AVAILABLE = True\n",
    "    print(\"Optuna available for hyperparameter optimization\")\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"Optuna not available - install with: pip install optuna\")\n",
    "\n",
    "# Additional utilities\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 2: Enhanced Market Data Download with Sentiment Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_enhanced_market_data():\n",
    "    \"\"\"Download VIX, VVIX and comprehensive market sentiment data\"\"\"\n",
    "    \n",
    "    # Core volatility indices\n",
    "    volatility_tickers = ['^VIX', '^VVIX']\n",
    "    \n",
    "    # Market sentiment indicators\n",
    "    sentiment_tickers = [\n",
    "        '^GSPC',    # S&P 500 (market direction)\n",
    "        '^IXIC',    # NASDAQ (tech sentiment)\n",
    "        '^RUT',     # Russell 2000 (small cap sentiment)\n",
    "        '^TNX',     # 10-Year Treasury (risk-off sentiment)\n",
    "        'DXY',      # US Dollar Index (risk sentiment)\n",
    "        'GLD',      # Gold ETF (safe haven)\n",
    "        'TLT',      # 20+ Year Treasury Bond ETF\n",
    "        'XLF',      # Financial Sector ETF (risk-on sentiment)\n",
    "        'QQQ',      # NASDAQ ETF (tech sentiment)\n",
    "        'IWM',      # Russell 2000 ETF (small cap sentiment)\n",
    "        'HYG',      # High Yield Corporate Bond ETF (credit risk)\n",
    "        'VXX',      # VIX ETF (volatility trading)\n",
    "        'SPY',      # S&P 500 ETF (market proxy)\n",
    "        'EFA',      # International developed markets\n",
    "        'EEM',      # Emerging markets\n",
    "        'USO',      # Oil ETF (commodity sentiment)\n",
    "    ]\n",
    "    \n",
    "    all_tickers = volatility_tickers + sentiment_tickers\n",
    "    start_date = '2010-01-01'\n",
    "    end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"Downloading enhanced market data from {start_date} to {end_date}...\")\n",
    "    print(f\"Core volatility tickers: {volatility_tickers}\")\n",
    "    print(f\"Sentiment tickers ({len(sentiment_tickers)}): {sentiment_tickers[:5]}...\")\n",
    "    \n",
    "    # Download all data with error handling\n",
    "    datasets = {}\n",
    "    failed_tickers = []\n",
    "    \n",
    "    for i, ticker in enumerate(all_tickers):\n",
    "        try:\n",
    "            print(f\"Downloading {ticker} ({i+1}/{len(all_tickers)})...\")\n",
    "            ticker_data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "            \n",
    "            if not ticker_data.empty and len(ticker_data) > 100:  # Minimum data requirement\n",
    "                datasets[ticker] = ticker_data\n",
    "                print(f\"  SUCCESS {ticker}: {ticker_data.shape[0]} days\")\n",
    "            else:\n",
    "                failed_tickers.append(ticker)\n",
    "                print(f\"  FAILED {ticker}: Insufficient data ({len(ticker_data)} days)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            failed_tickers.append(ticker)\n",
    "            print(f\"  ERROR {ticker}: Error - {str(e)[:50]}...\")\n",
    "    \n",
    "    if failed_tickers:\n",
    "        print(f\"\\nFailed to download: {failed_tickers}\")\n",
    "    \n",
    "    # Find common dates across all successful downloads\n",
    "    print(f\"\\nAligning data across {len(datasets)} successful downloads...\")\n",
    "    common_dates = None\n",
    "    for ticker, ticker_data in datasets.items():\n",
    "        if common_dates is None:\n",
    "            common_dates = ticker_data.index\n",
    "        else:\n",
    "            common_dates = common_dates.intersection(ticker_data.index)\n",
    "    \n",
    "    # Align all data by common dates\n",
    "    aligned_datasets = {}\n",
    "    for ticker, ticker_data in datasets.items():\n",
    "        aligned_datasets[ticker] = ticker_data.loc[common_dates]\n",
    "    \n",
    "    print(f\"\\n=== ENHANCED DATA SUMMARY ===\")\n",
    "    print(f\"Successfully downloaded: {len(aligned_datasets)} tickers\")\n",
    "    print(f\"Date range: {common_dates[0].strftime('%Y-%m-%d')} to {common_dates[-1].strftime('%Y-%m-%d')}\")\n",
    "    print(f\"Total trading days: {len(common_dates)}\")\n",
    "    print(f\"Core volatility data: {'SUCCESS' if '^VIX' in aligned_datasets else 'FAILED'}\")\n",
    "    print(f\"Sentiment indicators: {len([t for t in sentiment_tickers if t in aligned_datasets])}/{len(sentiment_tickers)}\")\n",
    "    \n",
    "    return aligned_datasets\n",
    "\n",
    "# Download enhanced market data\n",
    "print(\"Starting enhanced market data download...\")\n",
    "enhanced_market_data = download_enhanced_market_data()\n",
    "\n",
    "# Create enhanced combined dataset\n",
    "print(f\"\\nCreating combined dataset...\")\n",
    "enhanced_raw_data = pd.DataFrame()\n",
    "\n",
    "for ticker, data in enhanced_market_data.items():\n",
    "    # Add all OHLCV data with ticker suffix\n",
    "    ticker_clean = ticker.replace('^', '').replace('-', '_')\n",
    "    for col in data.columns:\n",
    "        enhanced_raw_data[f\"{col}_{ticker_clean}\"] = data[col]\n",
    "\n",
    "print(f\"\\nEnhanced dataset created:\")\n",
    "print(f\"   Shape: {enhanced_raw_data.shape}\")\n",
    "print(f\"   Columns: {enhanced_raw_data.shape[1]}\")\n",
    "print(f\"   Sample columns: {list(enhanced_raw_data.columns[:8])}\")\n",
    "print(f\"   Date range: {enhanced_raw_data.index[0]} to {enhanced_raw_data.index[-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 3: Enhanced Feature Engineering with Market Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment_features(enhanced_data):\n",
    "    \"\"\"Create comprehensive market sentiment features from multiple asset classes\"\"\"\n",
    "    sentiment_df = enhanced_data.copy()\n",
    "    \n",
    "    print(\"Creating market sentiment features...\")\n",
    "    \n",
    "    # Helper function to safely create ratios\n",
    "    def safe_ratio(numerator, denominator, default=1.0):\n",
    "        \"\"\"Create ratio with protection against division by zero\"\"\"\n",
    "        return np.where(denominator != 0, numerator / denominator, default)\n",
    "    \n",
    "    # 1. Risk-On/Risk-Off Indicators\n",
    "    if 'Close_GSPC' in sentiment_df.columns and 'Close_TLT' in sentiment_df.columns:\n",
    "        print(\"   Creating risk-on/risk-off indicators...\")\n",
    "        sentiment_df['Risk_On_Ratio'] = safe_ratio(sentiment_df['Close_GSPC'], sentiment_df['Close_TLT'])\n",
    "        sentiment_df['Risk_On_Ratio_MA20'] = sentiment_df['Risk_On_Ratio'].rolling(20).mean()\n",
    "        sentiment_df['Risk_On_Signal'] = (sentiment_df['Risk_On_Ratio'] > sentiment_df['Risk_On_Ratio_MA20']).astype(int)\n",
    "        sentiment_df['Risk_On_Momentum'] = sentiment_df['Risk_On_Ratio'].pct_change(5)\n",
    "    \n",
    "    # 2. Safe Haven Demand\n",
    "    if 'Close_GLD' in sentiment_df.columns and 'Close_GSPC' in sentiment_df.columns:\n",
    "        print(\"   Creating safe haven indicators...\")\n",
    "        sentiment_df['Safe_Haven_Ratio'] = safe_ratio(sentiment_df['Close_GLD'], sentiment_df['Close_GSPC'])\n",
    "        sentiment_df['Safe_Haven_MA20'] = sentiment_df['Safe_Haven_Ratio'].rolling(20).mean()\n",
    "        sentiment_df['Fear_Signal'] = (sentiment_df['Safe_Haven_Ratio'] > sentiment_df['Safe_Haven_MA20']).astype(int)\n",
    "        sentiment_df['Flight_to_Quality'] = sentiment_df['Safe_Haven_Ratio'].rolling(10).std()\n",
    "    \n",
    "    # 3. Credit Risk Indicators\n",
    "    if 'Close_HYG' in sentiment_df.columns and 'Close_TLT' in sentiment_df.columns:\n",
    "        print(\"   Creating credit risk indicators...\")\n",
    "        sentiment_df['Credit_Risk_Ratio'] = safe_ratio(sentiment_df['Close_HYG'], sentiment_df['Close_TLT'])\n",
    "        sentiment_df['Credit_Stress'] = sentiment_df['Credit_Risk_Ratio'].rolling(20).std()\n",
    "        sentiment_df['Credit_Spread_Proxy'] = -sentiment_df['Credit_Risk_Ratio'].pct_change(5)\n",
    "    \n",
    "    # 4. Sector Rotation Indicators\n",
    "    if 'Close_XLF' in sentiment_df.columns and 'Close_GSPC' in sentiment_df.columns:\n",
    "        print(\"   Creating sector rotation indicators...\")\n",
    "        sentiment_df['Financial_Strength'] = safe_ratio(sentiment_df['Close_XLF'], sentiment_df['Close_GSPC'])\n",
    "        sentiment_df['Financial_Momentum'] = sentiment_df['Financial_Strength'].pct_change(5)\n",
    "        sentiment_df['Financial_Outperformance'] = (sentiment_df['Financial_Momentum'] > 0).astype(int)\n",
    "    \n",
    "    # 5. Growth vs Value Sentiment\n",
    "    if 'Close_QQQ' in sentiment_df.columns and 'Close_IWM' in sentiment_df.columns:\n",
    "        print(\"   Creating growth vs value indicators...\")\n",
    "        sentiment_df['Growth_Value_Ratio'] = safe_ratio(sentiment_df['Close_QQQ'], sentiment_df['Close_IWM'])\n",
    "        sentiment_df['Growth_Preference'] = sentiment_df['Growth_Value_Ratio'].rolling(10).mean()\n",
    "        sentiment_df['Growth_Momentum'] = sentiment_df['Growth_Value_Ratio'].pct_change(3)\n",
    "    \n",
    "    # 6. Currency Sentiment\n",
    "    if 'Close_DXY' in sentiment_df.columns:\n",
    "        print(\"   Creating currency sentiment indicators...\")\n",
    "        sentiment_df['Dollar_Momentum_5d'] = sentiment_df['Close_DXY'].pct_change(5)\n",
    "        sentiment_df['Dollar_Momentum_20d'] = sentiment_df['Close_DXY'].pct_change(20)\n",
    "        sentiment_df['Dollar_Strength'] = (sentiment_df['Close_DXY'] > sentiment_df['Close_DXY'].rolling(50).mean()).astype(int)\n",
    "        sentiment_df['Dollar_Volatility'] = sentiment_df['Close_DXY'].rolling(10).std()\n",
    "    \n",
    "    # 7. Interest Rate Environment\n",
    "    if 'Close_TNX' in sentiment_df.columns:\n",
    "        print(\"   Creating interest rate indicators...\")\n",
    "        sentiment_df['Rate_Change_1d'] = sentiment_df['Close_TNX'].diff()\n",
    "        sentiment_df['Rate_Change_5d'] = sentiment_df['Close_TNX'].diff(5)\n",
    "        sentiment_df['Rate_Momentum'] = sentiment_df['Close_TNX'].pct_change(10)\n",
    "        sentiment_df['Rising_Rates'] = (sentiment_df['Rate_Change_5d'] > 0).astype(int)\n",
    "        sentiment_df['Rate_Volatility'] = sentiment_df['Close_TNX'].rolling(20).std()\n",
    "    \n",
    "    # 8. Cross-Asset Correlations\n",
    "    print(\"   Creating cross-asset correlations...\")\n",
    "    window = 20\n",
    "    \n",
    "    if 'Close_GSPC' in sentiment_df.columns and 'Close_VIX' in sentiment_df.columns:\n",
    "        sentiment_df['Stock_VIX_Corr'] = sentiment_df['Close_GSPC'].rolling(window).corr(sentiment_df['Close_VIX'])\n",
    "    \n",
    "    if 'Close_GLD' in sentiment_df.columns and 'Close_VIX' in sentiment_df.columns:\n",
    "        sentiment_df['Gold_VIX_Corr'] = sentiment_df['Close_GLD'].rolling(window).corr(sentiment_df['Close_VIX'])\n",
    "    \n",
    "    # 9. Market Regime Indicators\n",
    "    if 'Close_GSPC' in sentiment_df.columns:\n",
    "        print(\"   Creating market regime indicators...\")\n",
    "        sentiment_df['Bull_Market_50d'] = (sentiment_df['Close_GSPC'] > sentiment_df['Close_GSPC'].rolling(50).mean()).astype(int)\n",
    "        sentiment_df['Bull_Market_200d'] = (sentiment_df['Close_GSPC'] > sentiment_df['Close_GSPC'].rolling(200).mean()).astype(int)\n",
    "        sentiment_df['Market_Momentum_1d'] = sentiment_df['Close_GSPC'].pct_change(1)\n",
    "        sentiment_df['Market_Momentum_5d'] = sentiment_df['Close_GSPC'].pct_change(5)\n",
    "        sentiment_df['Market_Momentum_20d'] = sentiment_df['Close_GSPC'].pct_change(20)\n",
    "        sentiment_df['Market_Volatility_10d'] = sentiment_df['Market_Momentum_1d'].rolling(10).std()\n",
    "        sentiment_df['Market_Volatility_20d'] = sentiment_df['Market_Momentum_1d'].rolling(20).std()\n",
    "    \n",
    "    print(f\"Sentiment features created. New shape: {sentiment_df.shape}\")\n",
    "    return sentiment_df\n",
    "\n",
    "def create_enhanced_technical_features(df):\n",
    "    \"\"\"Enhanced technical features with sentiment integration\"\"\"\n",
    "    enhanced_df = df.copy()\n",
    "    \n",
    "    print(\"Creating enhanced technical features...\")\n",
    "    \n",
    "    def calculate_rsi(prices, window=14):\n",
    "        \"\"\"Calculate Relative Strength Index\"\"\"\n",
    "        delta = prices.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "    \n",
    "    # Enhanced VIX and VVIX features\n",
    "    for window in [5, 10, 20, 50, 100]:\n",
    "        if 'Close_VIX' in enhanced_df.columns:\n",
    "            print(f\"   Creating VIX features for {window}-day window...\")\n",
    "            enhanced_df[f'MA_{window}_VIX'] = enhanced_df['Close_VIX'].rolling(window).mean()\n",
    "            enhanced_df[f'STD_{window}_VIX'] = enhanced_df['Close_VIX'].rolling(window).std()\n",
    "            enhanced_df[f'RSI_{window}_VIX'] = calculate_rsi(enhanced_df['Close_VIX'], window)\n",
    "        \n",
    "        if 'Close_VVIX' in enhanced_df.columns:\n",
    "            enhanced_df[f'MA_{window}_VVIX'] = enhanced_df['Close_VVIX'].rolling(window).mean()\n",
    "            enhanced_df[f'STD_{window}_VVIX'] = enhanced_df['Close_VVIX'].rolling(window).std()\n",
    "    \n",
    "    # Enhanced yield calculations for multiple assets\n",
    "    print(\"   Creating yield and momentum features...\")\n",
    "    key_assets = ['VIX', 'VVIX', 'GSPC', 'TNX', 'GLD', 'DXY', 'QQQ', 'IWM', 'XLF', 'HYG']\n",
    "    \n",
    "    for asset in key_assets:\n",
    "        col = f'Close_{asset}'\n",
    "        if col in enhanced_df.columns:\n",
    "            # Multiple timeframe returns\n",
    "            enhanced_df[f'Return_1d_{asset}'] = enhanced_df[col].pct_change() * 100\n",
    "            enhanced_df[f'Return_3d_{asset}'] = enhanced_df[col].pct_change(3) * 100\n",
    "            enhanced_df[f'Return_5d_{asset}'] = enhanced_df[col].pct_change(5) * 100\n",
    "            enhanced_df[f'Return_10d_{asset}'] = enhanced_df[col].pct_change(10) * 100\n",
    "            \n",
    "            # Volatility measures\n",
    "            enhanced_df[f'Volatility_5d_{asset}'] = enhanced_df[f'Return_1d_{asset}'].rolling(5).std()\n",
    "            enhanced_df[f'Volatility_20d_{asset}'] = enhanced_df[f'Return_1d_{asset}'].rolling(20).std()\n",
    "            \n",
    "            # Price momentum\n",
    "            enhanced_df[f'Momentum_5d_{asset}'] = enhanced_df[col] / enhanced_df[col].shift(5) - 1\n",
    "            enhanced_df[f'Momentum_20d_{asset}'] = enhanced_df[col] / enhanced_df[col].shift(20) - 1\n",
    "    \n",
    "    # Enhanced lagged features for key indicators\n",
    "    print(\"   Creating lagged features...\")\n",
    "    for lag in [1, 2, 3, 5, 10, 20]:\n",
    "        for asset in ['VIX', 'VVIX', 'GSPC', 'TNX', 'GLD', 'DXY']:\n",
    "            col = f'Close_{asset}'\n",
    "            if col in enhanced_df.columns:\n",
    "                enhanced_df[f'Lag_{lag}_{asset}'] = enhanced_df[col].shift(lag)\n",
    "    \n",
    "    # VIX-specific features\n",
    "    if 'Close_VIX' in enhanced_df.columns:\n",
    "        print(\"   Creating VIX-specific features...\")\n",
    "        # VIX term structure (if VVIX available)\n",
    "        if 'Close_VVIX' in enhanced_df.columns:\n",
    "            enhanced_df['VIX_VVIX_Ratio'] = enhanced_df['Close_VIX'] / enhanced_df['Close_VVIX']\n",
    "            enhanced_df['VIX_VVIX_Spread'] = enhanced_df['Close_VVIX'] - enhanced_df['Close_VIX']\n",
    "        \n",
    "        # VIX regime indicators\n",
    "        enhanced_df['VIX_Regime_Low'] = (enhanced_df['Close_VIX'] < 15).astype(int)\n",
    "        enhanced_df['VIX_Regime_Normal'] = ((enhanced_df['Close_VIX'] >= 15) & (enhanced_df['Close_VIX'] < 25)).astype(int)\n",
    "        enhanced_df['VIX_Regime_High'] = ((enhanced_df['Close_VIX'] >= 25) & (enhanced_df['Close_VIX'] < 35)).astype(int)\n",
    "        enhanced_df['VIX_Regime_Extreme'] = (enhanced_df['Close_VIX'] >= 35).astype(int)\n",
    "        \n",
    "        # VIX mean reversion indicators\n",
    "        enhanced_df['VIX_Mean_Reversion_20'] = enhanced_df['Close_VIX'] / enhanced_df['Close_VIX'].rolling(20).mean()\n",
    "        enhanced_df['VIX_Mean_Reversion_50'] = enhanced_df['Close_VIX'] / enhanced_df['Close_VIX'].rolling(50).mean()\n",
    "    \n",
    "    print(f\"Enhanced technical features created. Shape: {enhanced_df.shape}\")\n",
    "    return enhanced_df\n",
    "\n",
    "# Apply enhanced feature engineering\n",
    "print(\"\\n=== ENHANCED FEATURE ENGINEERING ===\")\n",
    "sentiment_data = create_sentiment_features(enhanced_raw_data)\n",
    "enhanced_featured_data = create_enhanced_technical_features(sentiment_data)\n",
    "\n",
    "print(f\"\\nFeature engineering summary:\")\n",
    "print(f\"   Original enhanced data: {enhanced_raw_data.shape}\")\n",
    "print(f\"   With sentiment features: {sentiment_data.shape}\")\n",
    "print(f\"   Final enhanced features: {enhanced_featured_data.shape}\")\n",
    "print(f\"   Total features created: {enhanced_featured_data.shape[1]}\")\n",
    "print(f\"   Feature increase: {enhanced_featured_data.shape[1] - enhanced_raw_data.shape[1]} new features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 4: Data Cleaning and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_data_cleaning(df):\n",
    "    \"\"\"Enhanced cleaning with outlier handling and inf/NaN checks\"\"\"\n",
    "    clean_df = df.copy()\n",
    "    print(\"Starting data cleaning...\")\n",
    "    \n",
    "    # Replace inf with NaN\n",
    "    inf_count = np.isinf(clean_df.select_dtypes(include=[np.number])).sum().sum()\n",
    "    clean_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    print(f\"   Replaced {inf_count} infinite values\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    initial_na_count = clean_df.isnull().sum().sum()\n",
    "    clean_df = clean_df.fillna(method='ffill').fillna(method='bfill')\n",
    "    final_na_count = clean_df.isnull().sum().sum()\n",
    "    print(f\"   Filled {initial_na_count - final_na_count} missing values\")\n",
    "    \n",
    "    # Outlier winsorization\n",
    "    outlier_count = 0\n",
    "    for col in clean_df.select_dtypes(include=[np.number]).columns:\n",
    "        if clean_df[col].std() > 0:\n",
    "            upper_bound = clean_df[col].quantile(0.99)\n",
    "            lower_bound = clean_df[col].quantile(0.01)\n",
    "            outliers = (clean_df[col] > upper_bound) | (clean_df[col] < lower_bound)\n",
    "            clean_df.loc[clean_df[col] > upper_bound, col] = upper_bound\n",
    "            clean_df.loc[clean_df[col] < lower_bound, col] = lower_bound\n",
    "            outlier_count += outliers.sum()\n",
    "    \n",
    "    print(f\"   Winsorized {outlier_count} outliers\")\n",
    "    print(f\"Data cleaning completed. Shape: {clean_df.shape}\")\n",
    "    return clean_df\n",
    "\n",
    "def create_sequences(data, n_steps=30, target_col='Close_VIX'):\n",
    "    \"\"\"Create sequences for time series modeling\"\"\"\n",
    "    print(f\"Creating sequences with {n_steps} time steps...\")\n",
    "    \n",
    "    if target_col in data.columns:\n",
    "        cols = [col for col in data.columns if col != target_col] + [target_col]\n",
    "        data = data[cols]\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(n_steps, len(data)):\n",
    "        X.append(data.iloc[i-n_steps:i, :-1].values)\n",
    "        y.append(data.iloc[i, -1])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    print(f\"Created {len(X)} sequences with shape {X.shape}\")\n",
    "    return X, y\n",
    "\n",
    "def build_cnn_lstm_model(input_shape, learning_rate=0.001):\n",
    "    \"\"\"Build CNN-LSTM model\"\"\"\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        \n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        LSTM(64, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        LSTM(32),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_gru_model(input_shape, learning_rate=0.001):\n",
    "    \"\"\"Build GRU model\"\"\"\n",
    "    model = Sequential([\n",
    "        GRU(64, return_sequences=True, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        GRU(32),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Apply data cleaning and prepare for modeling\n",
    "print(\"\\n=== DATA PREPARATION ===\")\n",
    "cleaned_data = robust_data_cleaning(enhanced_featured_data)\n",
    "\n",
    "# Create sequences\n",
    "n_steps = 30\n",
    "X, y = create_sequences(cleaned_data, n_steps=n_steps, target_col='Close_VIX')\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"   Training: {X_train.shape}\")\n",
    "print(f\"   Testing: {X_test.shape}\")\n",
    "print(f\"   Features per timestep: {X_train.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 5: Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN-LSTM model\n",
    "print(\"\\n=== TRAINING CNN-LSTM MODEL ===\")\n",
    "cnn_lstm_model = build_cnn_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "print(f\"Model parameters: {cnn_lstm_model.count_params():,}\")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "cnn_lstm_history = cnn_lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train GRU model\n",
    "print(\"\\n=== TRAINING GRU MODEL ===\")\n",
    "gru_model = build_gru_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "print(f\"Model parameters: {gru_model.count_params():,}\")\n",
    "\n",
    "gru_history = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate models\n",
    "print(\"\\n=== MODEL EVALUATION ===\")\n",
    "\n",
    "# CNN-LSTM evaluation\n",
    "cnn_lstm_pred = cnn_lstm_model.predict(X_test, verbose=0)\n",
    "cnn_lstm_mse = mean_squared_error(y_test, cnn_lstm_pred)\n",
    "cnn_lstm_mae = mean_absolute_error(y_test, cnn_lstm_pred)\n",
    "cnn_lstm_r2 = r2_score(y_test, cnn_lstm_pred)\n",
    "cnn_lstm_rmse = np.sqrt(cnn_lstm_mse)\n",
    "\n",
    "# GRU evaluation\n",
    "gru_pred = gru_model.predict(X_test, verbose=0)\n",
    "gru_mse = mean_squared_error(y_test, gru_pred)\n",
    "gru_mae = mean_absolute_error(y_test, gru_pred)\n",
    "gru_r2 = r2_score(y_test, gru_pred)\n",
    "gru_rmse = np.sqrt(gru_mse)\n",
    "\n",
    "# Directional accuracy\n",
    "def calculate_directional_accuracy(y_true, y_pred):\n",
    "    y_true_diff = np.diff(y_true)\n",
    "    y_pred_diff = np.diff(y_pred.flatten())\n",
    "    correct_direction = np.sign(y_true_diff) == np.sign(y_pred_diff)\n",
    "    return np.mean(correct_direction) * 100\n",
    "\n",
    "cnn_lstm_dir_acc = calculate_directional_accuracy(y_test, cnn_lstm_pred)\n",
    "gru_dir_acc = calculate_directional_accuracy(y_test, gru_pred)\n",
    "\n",
    "print(\"\\n=== ENHANCED MODEL PERFORMANCE ===\")\n",
    "print(f\"\\nCNN-LSTM Results:\")\n",
    "print(f\"   MSE: {cnn_lstm_mse:.4f}\")\n",
    "print(f\"   RMSE: {cnn_lstm_rmse:.4f}\")\n",
    "print(f\"   MAE: {cnn_lstm_mae:.4f}\")\n",
    "print(f\"   R²: {cnn_lstm_r2:.4f}\")\n",
    "print(f\"   Directional Accuracy: {cnn_lstm_dir_acc:.1f}%\")\n",
    "\n",
    "print(f\"\\nGRU Results:\")\n",
    "print(f\"   MSE: {gru_mse:.4f}\")\n",
    "print(f\"   RMSE: {gru_rmse:.4f}\")\n",
    "print(f\"   MAE: {gru_mae:.4f}\")\n",
    "print(f\"   R²: {gru_r2:.4f}\")\n",
    "print(f\"   Directional Accuracy: {gru_dir_acc:.1f}%\")\n",
    "\n",
    "# Determine best model\n",
    "best_model_name = 'CNN-LSTM' if cnn_lstm_r2 > gru_r2 else 'GRU'\n",
    "best_r2 = max(cnn_lstm_r2, gru_r2)\n",
    "print(f\"\\nBest Model: {best_model_name} (R² = {best_r2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 6: Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "print(\"\\n=== CREATING VISUALIZATIONS ===\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Enhanced VIX Forecasting - Model Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "models = ['CNN-LSTM', 'GRU']\n",
    "r2_scores = [cnn_lstm_r2, gru_r2]\n",
    "colors = ['#2E86AB', '#A23B72']\n",
    "\n",
    "bars = axes[0, 0].bar(models, r2_scores, color=colors, alpha=0.8)\n",
    "axes[0, 0].set_title('R² Score Comparison', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('R² Score')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for bar, score in zip(bars, r2_scores):\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Prediction vs Actual (Best Model)\n",
    "best_predictions = cnn_lstm_pred if best_model_name == 'CNN-LSTM' else gru_pred\n",
    "axes[0, 1].scatter(y_test, best_predictions, alpha=0.6, c=colors[0])\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Actual VIX')\n",
    "axes[0, 1].set_ylabel('Predicted VIX')\n",
    "axes[0, 1].set_title(f'{best_model_name} Predictions vs Actual', fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Time Series Prediction Plot\n",
    "test_indices = range(len(y_test))\n",
    "axes[0, 2].plot(test_indices, y_test, label='Actual', color='black', linewidth=2)\n",
    "axes[0, 2].plot(test_indices, best_predictions.flatten(), label=f'{best_model_name} Predicted', \n",
    "                color=colors[0], linewidth=2, alpha=0.8)\n",
    "axes[0, 2].set_xlabel('Time Steps')\n",
    "axes[0, 2].set_ylabel('VIX Value')\n",
    "axes[0, 2].set_title('Time Series Predictions', fontweight='bold')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Training History (Best Model)\n",
    "best_history = cnn_lstm_history if best_model_name == 'CNN-LSTM' else gru_history\n",
    "axes[1, 0].plot(best_history.history['loss'], label='Training Loss', color=colors[0])\n",
    "axes[1, 0].plot(best_history.history['val_loss'], label='Validation Loss', color=colors[1])\n",
    "axes[1, 0].set_xlabel('Epochs')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].set_title(f'{best_model_name} Training History', fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Error Distribution\n",
    "errors = y_test - best_predictions.flatten()\n",
    "axes[1, 1].hist(errors, bins=30, alpha=0.7, color=colors[0], edgecolor='black')\n",
    "axes[1, 1].axvline(errors.mean(), color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean: {errors.mean():.3f}')\n",
    "axes[1, 1].set_xlabel('Prediction Error')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Error Distribution', fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Model Metrics Comparison\n",
    "metrics = ['MSE', 'MAE', 'RMSE', 'Dir. Acc.']\n",
    "cnn_lstm_metrics = [cnn_lstm_mse, cnn_lstm_mae, cnn_lstm_rmse, cnn_lstm_dir_acc/100]\n",
    "gru_metrics = [gru_mse, gru_mae, gru_rmse, gru_dir_acc/100]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 2].bar(x - width/2, cnn_lstm_metrics, width, label='CNN-LSTM', color=colors[0], alpha=0.8)\n",
    "axes[1, 2].bar(x + width/2, gru_metrics, width, label='GRU', color=colors[1], alpha=0.8)\n",
    "axes[1, 2].set_xlabel('Metrics')\n",
    "axes[1, 2].set_ylabel('Value')\n",
    "axes[1, 2].set_title('Model Metrics Comparison', fontweight='bold')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(metrics)\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comprehensive summary\n",
    "print(\"\\n=== COMPREHENSIVE ENHANCEMENT SUMMARY ===\")\n",
    "print(\"\\nData Enhancements:\")\n",
    "print(f\"   Market Sentiment: {len([t for t in enhanced_market_data.keys() if t not in ['^VIX', '^VVIX']])} indicators\")\n",
    "print(f\"   Feature Engineering: {enhanced_featured_data.shape[1] - enhanced_raw_data.shape[1]} new features\")\n",
    "print(f\"   Total Features: {enhanced_featured_data.shape[1]}\")\n",
    "print(f\"   Data Points: {len(cleaned_data):,}\")\n",
    "\n",
    "print(\"\\nModel Enhancements:\")\n",
    "print(f\"   Advanced Architectures: CNN-LSTM and GRU with batch normalization\")\n",
    "print(f\"   Regularization: Dropout and early stopping\")\n",
    "print(f\"   Sequence Length: {n_steps} time steps\")\n",
    "\n",
    "print(f\"\\nBest Model Performance ({best_model_name}):\")\n",
    "print(f\"   R² Score: {best_r2:.4f}\")\n",
    "print(f\"   RMSE: {cnn_lstm_rmse if best_model_name == 'CNN-LSTM' else gru_rmse:.4f}\")\n",
    "print(f\"   Directional Accuracy: {cnn_lstm_dir_acc if best_model_name == 'CNN-LSTM' else gru_dir_acc:.1f}%\")\n",
    "\n",
    "print(\"\\nEnhanced VIX forecasting analysis completed successfully!\")"
   ]
  }
 ],
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
   },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
