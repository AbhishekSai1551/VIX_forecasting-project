{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU Architecture Testing for VIX Forecasting\n",
    "\n",
    "This notebook implements comprehensive GRU architecture testing with:\n",
    "\n",
    "- **Multiple Architecture Variants**: Basic, Deep, Bidirectional, Attention, Residual, Dropout-Enhanced\n",
    "- **Hyperparameter Optimization**: Optuna-based optimization for each architecture\n",
    "- **Time Series Cross-Validation**: Proper temporal validation\n",
    "- **Statistical Significance Testing**: Robust model comparison\n",
    "- **Comprehensive Training & Evaluation**: Full pipeline for each variant\n",
    "- **Results Storage**: Systematic storage of all results for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shared utilities\n",
    "from vix_research_utils import *\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, GRU, Input, MultiHeadAttention, LayerNormalization,\n",
    "    Bidirectional, BatchNormalization, GlobalAveragePooling1D, Add,\n",
    "    GaussianNoise, Concatenate\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "\n",
    "# Additional imports\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 2: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data using shared utilities\n",
    "print(\"Loading VIX and VVIX data...\")\n",
    "vix_data, vvix_data = download_market_data()\n",
    "\n",
    "print(\"Creating features...\")\n",
    "features_df = create_features(vix_data, vvix_data)\n",
    "\n",
    "print(\"Preparing sequences for deep learning...\")\n",
    "X, y, feature_names, scaler = prepare_sequences(features_df, sequence_length=30)\n",
    "\n",
    "# Split data for time series\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 3: GRU Architecture Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_basic_gru(trial, input_shape):\n",
    "    \"\"\"Basic GRU architecture\"\"\"\n",
    "    gru_units_1 = trial.suggest_int('gru_units_1', 32, 128, step=32)\n",
    "    gru_units_2 = trial.suggest_int('gru_units_2', 16, 64, step=16)\n",
    "    dense_units = trial.suggest_int('dense_units', 8, 32, step=8)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        GRU(gru_units_1, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        GRU(gru_units_2),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_deep_gru(trial, input_shape):\n",
    "    \"\"\"Deep GRU with multiple layers\"\"\"\n",
    "    gru_units_1 = trial.suggest_int('gru_units_1', 64, 256, step=64)\n",
    "    gru_units_2 = trial.suggest_int('gru_units_2', 32, 128, step=32)\n",
    "    gru_units_3 = trial.suggest_int('gru_units_3', 16, 64, step=16)\n",
    "    dense_units = trial.suggest_int('dense_units', 16, 64, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.6)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        GRU(gru_units_1, return_sequences=True, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        GRU(gru_units_2, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        GRU(gru_units_3),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_bidirectional_gru(trial, input_shape):\n",
    "    \"\"\"Bidirectional GRU\"\"\"\n",
    "    gru_units_1 = trial.suggest_int('gru_units_1', 32, 128, step=32)\n",
    "    gru_units_2 = trial.suggest_int('gru_units_2', 16, 64, step=16)\n",
    "    dense_units = trial.suggest_int('dense_units', 16, 64, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Bidirectional(GRU(gru_units_1, return_sequences=True), input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        Bidirectional(GRU(gru_units_2)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Architecture registry\n",
    "GRU_ARCHITECTURES = {\n",
    "    'Basic_GRU': build_basic_gru,\n",
    "    'Deep_GRU': build_deep_gru,\n",
    "    'Bidirectional_GRU': build_bidirectional_gru\n",
    "}\n",
    "\n",
    "print(f\"GRU architectures available: {list(GRU_ARCHITECTURES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 4: Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(trial, architecture_name, X_train, y_train, input_shape):\n",
    "    \"\"\"Objective function for hyperparameter optimization\"\"\"\n",
    "    try:\n",
    "        model_builder = GRU_ARCHITECTURES[architecture_name]\n",
    "        model = model_builder(trial, input_shape)\n",
    "        \n",
    "        # Time series cross-validation\n",
    "        tscv = TimeSeriesSplit(n_splits=3)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in tscv.split(X_train):\n",
    "            X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "            \n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_fold_train, y_fold_train,\n",
    "                validation_data=(X_fold_val, y_fold_val),\n",
    "                epochs=50,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stopping, reduce_lr],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            val_loss = min(history.history['val_loss'])\n",
    "            cv_scores.append(val_loss)\n",
    "            tf.keras.backend.clear_session()\n",
    "        \n",
    "        return np.mean(cv_scores)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "def optimize_architecture(architecture_name, X_train, y_train, input_shape, n_trials=50):\n",
    "    \"\"\"Optimize hyperparameters for a specific architecture\"\"\"\n",
    "    print(f\"Optimizing {architecture_name}...\")\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        study_name=f'{architecture_name}_optimization',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42)\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: objective_function(trial, architecture_name, X_train, y_train, input_shape),\n",
    "        n_trials=n_trials\n",
    "    )\n",
    "    \n",
    "    print(f\"Best trial for {architecture_name}: Value={study.best_trial.value:.6f}\")\n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 5: Execute Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute comprehensive testing for all GRU architectures\n",
    "print(\"Starting GRU Architecture Testing...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_results = {}\n",
    "all_studies = {}\n",
    "\n",
    "for architecture_name in GRU_ARCHITECTURES.keys():\n",
    "    print(f\"Testing {architecture_name}...\")\n",
    "    \n",
    "    # Optimize hyperparameters\n",
    "    study = optimize_architecture(architecture_name, X_train, y_train, input_shape, n_trials=25)\n",
    "    all_studies[architecture_name] = study\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    best_params = study.best_trial.params\n",
    "    \n",
    "    # Create mock trial for final training\n",
    "    class MockTrial:\n",
    "        def __init__(self, params):\n",
    "            self.params = params\n",
    "        def suggest_int(self, name, low, high, step=1):\n",
    "            return self.params.get(name, low)\n",
    "        def suggest_float(self, name, low, high, log=False):\n",
    "            return self.params.get(name, low)\n",
    "    \n",
    "    mock_trial = MockTrial(best_params)\n",
    "    model_builder = GRU_ARCHITECTURES[architecture_name]\n",
    "    final_model = model_builder(mock_trial, input_shape)\n",
    "    \n",
    "    # Train final model\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "    \n",
    "    history = final_model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Make predictions and calculate metrics\n",
    "    train_pred = final_model.predict(X_train, verbose=0).flatten()\n",
    "    test_pred = final_model.predict(X_test, verbose=0).flatten()\n",
    "    \n",
    "    train_metrics = calculate_metrics(y_train, train_pred)\n",
    "    test_metrics = calculate_metrics(y_test, test_pred)\n",
    "    \n",
    "    results = {\n",
    "        'architecture': architecture_name,\n",
    "        'best_params': best_params,\n",
    "        'model': final_model,\n",
    "        'history': history,\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'train_predictions': train_pred,\n",
    "        'test_predictions': test_pred\n",
    "    }\n",
    "    \n",
    "    all_results[architecture_name] = results\n",
    "    \n",
    "    print(f\"Final {architecture_name} Results:\")\n",
    "    print(f\"  Test MSE: {test_metrics['MSE']:.6f}\")\n",
    "    print(f\"  Test R²: {test_metrics['R2']:.6f}\")\n",
    "    print(f\"Completed {architecture_name}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Save comprehensive results\n",
    "comprehensive_results = {\n",
    "    'results': all_results,\n",
    "    'studies': all_studies,\n",
    "    'data_info': {\n",
    "        'input_shape': input_shape,\n",
    "        'train_samples': X_train.shape[0],\n",
    "        'test_samples': X_test.shape[0],\n",
    "        'features': feature_names\n",
    "    }\n",
    "}\n",
    "\n",
    "save_model_results(comprehensive_results, 'gru_comprehensive_results.pkl')\n",
    "print(\"GRU Architecture Testing Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}