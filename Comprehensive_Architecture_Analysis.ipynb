{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Architecture Analysis for VIX Forecasting\n",
    "\n",
    "This notebook provides comprehensive analysis and comparison of all CNN-LSTM and GRU architectures with:\n",
    "- **Statistical Significance Testing**: Robust comparison between all models\n",
    "- **Performance Metrics Analysis**: Detailed evaluation of all architectures\n",
    "- **Confidence Intervals**: Statistical confidence in predictions\n",
    "- **Walk-Forward Validation**: Time series specific validation\n",
    "- **Economic Significance Analysis**: Real-world trading implications\n",
    "- **Best Model Selection**: Data-driven architecture selection\n",
    "\n",
    "## Architectures Analyzed:\n",
    "\n",
    "### CNN-LSTM Variants:\n",
    "1. **Basic CNN-LSTM**: Baseline hybrid architecture\n",
    "2. **Deep CNN-LSTM**: Enhanced depth with multiple layers\n",
    "3. **Bidirectional CNN-LSTM**: Bidirectional temporal modeling\n",
    "4. **Attention CNN-LSTM**: Multi-head attention mechanism\n",
    "5. **Multiscale CNN-LSTM**: Multi-scale feature extraction\n",
    "\n",
    "### GRU Variants:\n",
    "1. **Basic GRU**: Baseline recurrent architecture\n",
    "2. **Deep GRU**: Enhanced depth with multiple layers\n",
    "3. **Bidirectional GRU**: Bidirectional temporal modeling\n",
    "4. **Attention GRU**: Multi-head attention mechanism\n",
    "5. **Residual GRU**: Residual connections for gradient flow\n",
    "6. **Dropout-Enhanced GRU**: Advanced regularization strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shared utilities\n",
    "from vix_research_utils import *\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel, wilcoxon, friedmanchisquare\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from arch.unitroot import ADF\n",
    "\n",
    "# Additional imports\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 2: Load Results and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for analysis\n",
    "print(\"Loading VIX and VVIX data...\")\n",
    "vix_data, vvix_data = download_market_data()\n",
    "vix_clean = clean_data(vix_data)\n",
    "vvix_clean = clean_data(vvix_data)\n",
    "features_df = create_features(vix_clean, vvix_clean)\n",
    "X, y, feature_names, scaler = prepare_sequences(features_df, sequence_length=30)\n",
    "\n",
    "# Split data\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Data loaded: {X.shape[0]} samples, {X.shape[2]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Load CNN-LSTM results\n",
    "cnn_lstm_results = {}\n",
    "cnn_lstm_architectures = ['Basic_CNN_LSTM', 'Deep_CNN_LSTM', 'Bidirectional_CNN_LSTM', \n",
    "                         'Attention_CNN_LSTM', 'Multiscale_CNN_LSTM']\n",
    "\n",
    "for arch in cnn_lstm_architectures:\n",
    "    try:\n",
    "        results = joblib.load(f'cnn_lstm_results_{arch.lower()}.pkl')\n",
    "        cnn_lstm_results[arch] = results\n",
    "        print(f\"Loaded {arch} results\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {arch} results not found\")\n",
    "\n",
    "# Load GRU results\n",
    "gru_results = {}\n",
    "gru_architectures = ['Basic_GRU', 'Deep_GRU', 'Bidirectional_GRU', \n",
    "                    'Attention_GRU', 'Residual_GRU', 'Dropout_Enhanced_GRU']\n",
    "\n",
    "for arch in gru_architectures:\n",
    "    try:\n",
    "        results = joblib.load(f'gru_results_{arch.lower()}.pkl')\n",
    "        gru_results[arch] = results\n",
    "        print(f\"Loaded {arch} results\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {arch} results not found\")\n",
    "\n",
    "# Combine all results\n",
    "all_results = {**cnn_lstm_results, **gru_results}\n",
    "print(f\"\\nTotal architectures loaded: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 3: Performance Metrics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_summary(results_dict):\n",
    "    \"\"\"Create comprehensive performance summary\"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for arch_name, results in results_dict.items():\n",
    "        if results is None:\n",
    "            continue\n",
    "            \n",
    "        test_metrics = results['test_metrics']\n",
    "        train_metrics = results['train_metrics']\n",
    "        \n",
    "        # Calculate additional metrics\n",
    "        test_predictions = results['test_predictions']\n",
    "        train_predictions = results['train_predictions']\n",
    "        \n",
    "        # Overfitting measure\n",
    "        overfitting = train_metrics['mse'] - test_metrics['mse']\n",
    "        \n",
    "        # Prediction variance\n",
    "        pred_variance = np.var(test_predictions)\n",
    "        \n",
    "        # Architecture type\n",
    "        arch_type = 'CNN-LSTM' if 'CNN_LSTM' in arch_name else 'GRU'\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Architecture': arch_name,\n",
    "            'Type': arch_type,\n",
    "            'Test_MSE': test_metrics['mse'],\n",
    "            'Test_MAE': test_metrics['mae'],\n",
    "            'Test_R2': test_metrics['r2'],\n",
    "            'Train_MSE': train_metrics['mse'],\n",
    "            'Train_MAE': train_metrics['mae'],\n",
    "            'Train_R2': train_metrics['r2'],\n",
    "            'Overfitting': overfitting,\n",
    "            'Pred_Variance': pred_variance,\n",
    "            'Best_Params': str(results['best_params'])\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(summary_data)\n",
    "\n",
    "# Create performance summary\n",
    "performance_df = create_performance_summary(all_results)\n",
    "\n",
    "if not performance_df.empty:\n",
    "    # Sort by test MSE (lower is better)\n",
    "    performance_df = performance_df.sort_values('Test_MSE')\n",
    "    \n",
    "    print(\"PERFORMANCE SUMMARY (sorted by Test MSE)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Display key metrics\n",
    "    display_cols = ['Architecture', 'Type', 'Test_MSE', 'Test_MAE', 'Test_R2', 'Overfitting']\n",
    "    print(performance_df[display_cols].to_string(index=False, float_format='%.6f'))\n",
    "    \n",
    "    # Best performing model\n",
    "    best_model = performance_df.iloc[0]\n",
    "    print(f\"\\nBEST PERFORMING MODEL: {best_model['Architecture']}\")\n",
    "    print(f\"Test MSE: {best_model['Test_MSE']:.6f}\")\n",
    "    print(f\"Test MAE: {best_model['Test_MAE']:.6f}\")\n",
    "    print(f\"Test R²: {best_model['Test_R2']:.6f}\")\nelse:\n    print(\"No results available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 4: Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_tests(results_dict, y_test):\n",
    "    \"\"\"Perform comprehensive statistical significance testing\"\"\"\n",
    "    if len(results_dict) < 2:\n",
    "        print(\"Need at least 2 models for statistical comparison\")\n",
    "        return None\n",
    "    \n",
    "    # Extract predictions and calculate errors\n",
    "    model_errors = {}\n",
    "    model_predictions = {}\n",
    "    \n",
    "    for arch_name, results in results_dict.items():\n",
    "        if results is None:\n",
    "            continue\n",
    "        predictions = results['test_predictions']\n",
    "        errors = np.abs(predictions - y_test)\n",
    "        model_errors[arch_name] = errors\n",
    "        model_predictions[arch_name] = predictions\n",
    "    \n",
    "    print(\"STATISTICAL SIGNIFICANCE TESTING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Pairwise t-tests (paired samples)\n",
    "    print(\"\\n1. PAIRWISE T-TESTS (Paired Samples)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    model_names = list(model_errors.keys())\n",
    "    pairwise_results = []\n",
    "    \n",
    "    for i, model1 in enumerate(model_names):\n",
    "        for j, model2 in enumerate(model_names[i+1:], i+1):\n",
    "            errors1 = model_errors[model1]\n",
    "            errors2 = model_errors[model2]\n",
    "            \n",
    "            # Paired t-test\n",
    "            t_stat, p_value = ttest_rel(errors1, errors2)\n",
    "            \n",
    "            # Effect size (Cohen's d)\n",
    "            diff = errors1 - errors2\n",
    "            cohens_d = np.mean(diff) / np.std(diff)\n",
    "            \n",
    "            pairwise_results.append({\n",
    "                'Model1': model1,\n",
    "                'Model2': model2,\n",
    "                'T_Statistic': t_stat,\n",
    "                'P_Value': p_value,\n",
    "                'Cohens_D': cohens_d,\n",
    "                'Significant': p_value < 0.05\n",
    "            })\n",
    "            \n",
    "            significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"ns\"\n",
    "            print(f\"{model1} vs {model2}: t={t_stat:.3f}, p={p_value:.6f} {significance}\")\n",
    "    \n",
    "    # Friedman test (non-parametric)\n",
    "    print(\"\\n2. FRIEDMAN TEST (Non-parametric)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if len(model_errors) >= 3:\n",
    "        error_arrays = [model_errors[name] for name in model_names]\n",
    "        friedman_stat, friedman_p = friedmanchisquare(*error_arrays)\n",
    "        print(f\"Friedman statistic: {friedman_stat:.6f}\")\n",
    "        print(f\"P-value: {friedman_p:.6f}\")\n",
    "        print(f\"Significant difference: {'Yes' if friedman_p < 0.05 else 'No'}\")\n",
    "    else:\n",
    "        print(\"Need at least 3 models for Friedman test\")\n",
    "    \n",
    "    return pd.DataFrame(pairwise_results)\n",
    "\n",
    "# Perform statistical tests\n",
    "if len(all_results) >= 2:\n",
    "    statistical_results = perform_statistical_tests(all_results, y_test)\nelse:\n    print(\"Need at least 2 models for statistical testing\")\n    statistical_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 5: Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_visualizations(results_dict, performance_df, y_test):\n",
    "    \"\"\"Create comprehensive visualizations for model comparison\"\"\"\n",
    "    if len(results_dict) == 0:\n",
    "        print(\"No results available for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Set up the plotting environment\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Performance Comparison Bar Plot\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    if not performance_df.empty:\n",
    "        sns.barplot(data=performance_df, x='Test_MSE', y='Architecture', hue='Type', ax=ax1)\n",
    "        ax1.set_title('Test MSE Comparison by Architecture', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Test MSE (lower is better)')\n",
    "    \n",
    "    # 2. R² Comparison\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    if not performance_df.empty:\n",
    "        sns.barplot(data=performance_df, x='Test_R2', y='Architecture', hue='Type', ax=ax2)\n",
    "        ax2.set_title('Test R² Comparison by Architecture', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Test R² (higher is better)')\n",
    "    \n",
    "    # 3. Overfitting Analysis\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    if not performance_df.empty:\n",
    "        sns.scatterplot(data=performance_df, x='Train_MSE', y='Test_MSE', \n",
    "                       hue='Type', size='Overfitting', ax=ax3)\n",
    "        # Add diagonal line for reference\n",
    "        min_val = min(performance_df['Train_MSE'].min(), performance_df['Test_MSE'].min())\n",
    "        max_val = max(performance_df['Train_MSE'].max(), performance_df['Test_MSE'].max())\n",
    "        ax3.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)\n",
    "        ax3.set_title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Train MSE')\n",
    "        ax3.set_ylabel('Test MSE')\n",
    "    \n",
    "    # 4. Prediction Distribution\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    for arch_name, results in results_dict.items():\n",
    "        if results is not None:\n",
    "            predictions = results['test_predictions']\n",
    "            ax4.hist(predictions, alpha=0.6, label=arch_name, bins=20)\n",
    "    ax4.hist(y_test, alpha=0.8, label='Actual', bins=20, color='black')\n",
    "    ax4.set_title('Prediction Distributions', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('VIX Value')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # 5. Error Distribution Boxplot\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    error_data = []\n",
    "    for arch_name, results in results_dict.items():\n",
    "        if results is not None:\n",
    "            predictions = results['test_predictions']\n",
    "            errors = np.abs(predictions - y_test)\n",
    "            for error in errors:\n",
    "                error_data.append({'Architecture': arch_name, 'Absolute_Error': error})\n",
    "    \n",
    "    if error_data:\n",
    "        error_df = pd.DataFrame(error_data)\n",
    "        sns.boxplot(data=error_df, x='Absolute_Error', y='Architecture', ax=ax5)\n",
    "        ax5.set_title('Error Distribution by Architecture', fontsize=14, fontweight='bold')\n",
    "        ax5.set_xlabel('Absolute Error')\n",
    "    \n",
    "    # 6. Time Series Predictions (Best vs Worst)\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    if not performance_df.empty and len(results_dict) >= 2:\n",
    "        best_arch = performance_df.iloc[0]['Architecture']\n",
    "        worst_arch = performance_df.iloc[-1]['Architecture']\n",
    "        \n",
    "        # Plot last 100 predictions\n",
    "        n_plot = min(100, len(y_test))\n",
    "        x_range = range(len(y_test) - n_plot, len(y_test))\n",
    "        \n",
    "        ax6.plot(x_range, y_test[-n_plot:], 'k-', label='Actual', linewidth=2)\n",
    "        \n",
    "        if best_arch in results_dict and results_dict[best_arch] is not None:\n",
    "            best_pred = results_dict[best_arch]['test_predictions']\n",
    "            ax6.plot(x_range, best_pred[-n_plot:], '--', label=f'Best: {best_arch}', linewidth=2)\n",
    "        \n",
    "        if worst_arch in results_dict and results_dict[worst_arch] is not None:\n",
    "            worst_pred = results_dict[worst_arch]['test_predictions']\n",
    "            ax6.plot(x_range, worst_pred[-n_plot:], ':', label=f'Worst: {worst_arch}', linewidth=2)\n",
    "        \n",
    "        ax6.set_title('Time Series Predictions (Last 100 points)', fontsize=14, fontweight='bold')\n",
    "        ax6.set_xlabel('Time Index')\n",
    "        ax6.set_ylabel('VIX Value')\n",
    "        ax6.legend()\n",
    "        ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "if len(all_results) > 0:\n",
    "    create_comprehensive_visualizations(all_results, performance_df, y_test)\n",
    "else:\n",
    "    print(\"No results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 6: Economic Significance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_economic_significance(results_dict, y_test, threshold=0.5):\n",
    "    \"\"\"Analyze economic significance of predictions\"\"\"\n",
    "    print(\"ECONOMIC SIGNIFICANCE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    economic_results = []\n",
    "    \n",
    "    for arch_name, results in results_dict.items():\n",
    "        if results is None:\n",
    "            continue\n",
    "            \n",
    "        predictions = results['test_predictions']\n",
    "        \n",
    "        # Direction accuracy (up/down prediction)\n",
    "        actual_direction = np.diff(y_test) > 0\n",
    "        pred_direction = np.diff(predictions) > 0\n",
    "        direction_accuracy = np.mean(actual_direction == pred_direction)\n",
    "        \n",
    "        # Large movement prediction (>threshold)\n",
    "        large_movements = np.abs(np.diff(y_test)) > threshold\n",
    "        if np.sum(large_movements) > 0:\n",
    "            large_move_accuracy = np.mean(\n",
    "                actual_direction[large_movements] == pred_direction[large_movements]\n",
    "            )\n",
    "        else:\n",
    "            large_move_accuracy = np.nan\n",
    "        \n",
    "        # Volatility regime prediction\n",
    "        high_vol_regime = y_test > np.percentile(y_test, 75)\n",
    "        low_vol_regime = y_test < np.percentile(y_test, 25)\n",
    "        \n",
    "        high_vol_mse = np.mean((predictions[high_vol_regime] - y_test[high_vol_regime])**2)\n",
    "        low_vol_mse = np.mean((predictions[low_vol_regime] - y_test[low_vol_regime])**2)\n",
    "        \n",
    "        # Trading signal accuracy (simplified)\n",
    "        pred_changes = np.diff(predictions)\n",
    "        actual_changes = np.diff(y_test)\n",
    "        \n",
    "        # Profitable trades (same direction)\n",
    "        profitable_trades = np.sum((pred_changes * actual_changes) > 0)\n",
    "        total_trades = len(pred_changes)\n",
    "        profit_ratio = profitable_trades / total_trades if total_trades > 0 else 0\n",
    "        \n",
    "        economic_results.append({\n",
    "            'Architecture': arch_name,\n",
    "            'Direction_Accuracy': direction_accuracy,\n",
    "            'Large_Move_Accuracy': large_move_accuracy,\n",
    "            'High_Vol_MSE': high_vol_mse,\n",
    "            'Low_Vol_MSE': low_vol_mse,\n",
    "            'Profit_Ratio': profit_ratio\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{arch_name}:\")\n",
    "        print(f\"  Direction Accuracy: {direction_accuracy:.3f}\")\n",
    "        print(f\"  Large Move Accuracy: {large_move_accuracy:.3f}\")\n",
    "        print(f\"  High Vol MSE: {high_vol_mse:.6f}\")\n",
    "        print(f\"  Low Vol MSE: {low_vol_mse:.6f}\")\n",
    "        print(f\"  Profit Ratio: {profit_ratio:.3f}\")\n",
    "    \n",
    "    return pd.DataFrame(economic_results)\n",
    "\n",
    "# Perform economic significance analysis\n",
    "if len(all_results) > 0:\n",
    "    economic_df = analyze_economic_significance(all_results, y_test)\n",
    "else:\n",
    "    print(\"No results available for economic analysis\")\n",
    "    economic_df = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
