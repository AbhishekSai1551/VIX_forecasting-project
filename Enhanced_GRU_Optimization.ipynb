{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced GRU Hyperparameter Optimization for VIX Forecasting\n",
    "\n",
    "This notebook implements enhanced GRU optimization with:\n",
    "- Time series cross-validation\n",
    "- Statistical significance testing\n",
    "- Expanded search spaces\n",
    "- Confidence intervals\n",
    "- Baseline model comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shared utilities\n",
    "from vix_research_utils import *\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, GRU, Input, MultiHeadAttention, LayerNormalization,\n",
    "    Bidirectional, BatchNormalization, GlobalAveragePooling1D, Add\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Additional imports\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "print(\"Enhanced GRU optimization setup completed\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Optuna version: {optuna.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 2: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data using shared utilities\n",
    "print(\"Preparing data...\")\n",
    "vix_raw, vvix_raw = download_market_data()\n",
    "raw_data = pd.merge(vix_raw, vvix_raw, left_index=True, right_index=True, suffixes=('_VIX', '_VVIX'))\n",
    "cleaned_data = clean_data(raw_data)\n",
    "featured_data = create_technical_features(cleaned_data)\n",
    "optimized_data, pca_model, scaler, selected_features = optimize_features(featured_data)\n",
    "\n",
    "# Create sequences\n",
    "n_steps = 30\n",
    "X, y = create_sequences(optimized_data, n_steps)\n",
    "\n",
    "# Enhanced data splits\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_val = X[train_size:train_size+val_size]\n",
    "y_val = y[train_size:train_size+val_size]\n",
    "X_test = X[train_size+val_size:]\n",
    "y_test = y[train_size+val_size:]\n",
    "\n",
    "print(f\"Data prepared successfully\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "print(f\"Features per timestep: {X_train.shape[2]}\")\n",
    "\n",
    "# Create time series CV splits\n",
    "cv_splits = time_series_split(X_train, y_train, n_splits=5)\n",
    "print(f\"Created {len(cv_splits)} time series CV splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 3: Enhanced GRU Architecture Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_enhanced_basic_gru(trial, input_shape):\n",
    "    \"\"\"Enhanced basic GRU with expanded hyperparameter space\"\"\"\n",
    "    # Expanded GRU hyperparameters\n",
    "    gru_units_1 = trial.suggest_int('gru_units_1', 16, 256, step=16)\n",
    "    gru_units_2 = trial.suggest_int('gru_units_2', 8, 128, step=8)\n",
    "    \n",
    "    # Enhanced regularization\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.05, 0.6, step=0.05)\n",
    "    recurrent_dropout = trial.suggest_float('recurrent_dropout', 0.0, 0.5, step=0.05)\n",
    "    \n",
    "    # Dense layer\n",
    "    dense_units = trial.suggest_int('dense_units', 4, 64, step=4)\n",
    "    \n",
    "    # Enhanced optimizer parameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        GRU(gru_units_1, return_sequences=True, recurrent_dropout=recurrent_dropout, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        GRU(gru_units_2, recurrent_dropout=recurrent_dropout),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_enhanced_bidirectional_gru(trial, input_shape):\n",
    "    \"\"\"Enhanced bidirectional GRU with expanded search space\"\"\"\n",
    "    # Enhanced GRU hyperparameters\n",
    "    gru_units_1 = trial.suggest_int('gru_units_1', 16, 192, step=16)\n",
    "    gru_units_2 = trial.suggest_int('gru_units_2', 8, 96, step=8)\n",
    "    \n",
    "    # Enhanced regularization\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.05, 0.5, step=0.05)\n",
    "    recurrent_dropout = trial.suggest_float('recurrent_dropout', 0.0, 0.4, step=0.05)\n",
    "    \n",
    "    # Dense layer\n",
    "    dense_units = trial.suggest_int('dense_units', 8, 96, step=4)\n",
    "    \n",
    "    # Enhanced optimizer\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    \n",
    "    model = Sequential([\n",
    "        Bidirectional(GRU(gru_units_1, return_sequences=True, recurrent_dropout=recurrent_dropout), input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Bidirectional(GRU(gru_units_2, recurrent_dropout=recurrent_dropout)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "        \n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_enhanced_attention_gru(trial, input_shape):\n",
    "    \"\"\"Enhanced GRU with attention and expanded search space\"\"\"\n",
    "    # Enhanced GRU hyperparameters\n",
    "    gru_units_1 = trial.suggest_int('gru_units_1', 16, 192, step=16)\n",
    "    gru_units_2 = trial.suggest_int('gru_units_2', 8, 96, step=8)\n",
    "    \n",
    "    # Enhanced attention hyperparameters\n",
    "    num_heads = trial.suggest_int('num_heads', 1, 12)\n",
    "    key_dim = trial.suggest_int('key_dim', 4, 64, step=4)\n",
    "    \n",
    "    # Enhanced regularization\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.05, 0.5, step=0.05)\n",
    "    attention_dropout = trial.suggest_float('attention_dropout', 0.0, 0.3, step=0.05)\n",
    "    \n",
    "    # Dense layer\n",
    "    dense_units = trial.suggest_int('dense_units', 8, 96, step=4)\n",
    "    \n",
    "    # Enhanced optimizer\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # GRU layers\n",
    "    gru_out = GRU(gru_units_1, return_sequences=True)(inputs)\n",
    "    gru_out = BatchNormalization()(gru_out)\n",
    "    gru_out = Dropout(dropout_rate)(gru_out)\n",
    "    \n",
    "    gru_out2 = GRU(gru_units_2, return_sequences=True)(gru_out)\n",
    "    gru_out2 = BatchNormalization()(gru_out2)\n",
    "    gru_out2 = Dropout(dropout_rate)(gru_out2)\n",
    "    \n",
    "    # Enhanced attention mechanism\n",
    "    attention = MultiHeadAttention(\n",
    "        num_heads=num_heads, \n",
    "        key_dim=key_dim,\n",
    "        dropout=attention_dropout\n",
    "    )(gru_out2, gru_out2)\n",
    "    attention = LayerNormalization()(attention + gru_out2)\n",
    "    \n",
    "    # Global pooling\n",
    "    pooled = GlobalAveragePooling1D()(attention)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(dense_units, activation='relu')(pooled)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(dense_units // 2, activation='relu')(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Architecture registry\n",
    "ENHANCED_GRU_ARCHITECTURES = {\n",
    "    'Enhanced_Basic_GRU': build_enhanced_basic_gru,\n",
    "    'Enhanced_Bidirectional_GRU': build_enhanced_bidirectional_gru,\n",
    "    'Enhanced_Attention_GRU': build_enhanced_attention_gru\n",
    "}\n",
    "\n",
    "print(f\"Enhanced GRU architectures defined: {list(ENHANCED_GRU_ARCHITECTURES.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 4: Enhanced Optimization Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_gru_cv_objective(trial, architecture_name, input_shape, X, y, cv_splits):\n",
    "    \"\"\"Enhanced GRU objective function with time series cross-validation\"\"\"\n",
    "    model_builder = ENHANCED_GRU_ARCHITECTURES[architecture_name]\n",
    "    \n",
    "    # Enhanced training parameters\n",
    "    batch_size = trial.suggest_categorical('batch_size', [4, 8, 16, 32, 64, 128])\n",
    "    epochs = trial.suggest_int('epochs', 20, 150, step=10)\n",
    "    patience = trial.suggest_int('patience', 5, 25, step=2)\n",
    "    \n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv_splits):\n",
    "        try:\n",
    "            X_train_fold = X[train_idx]\n",
    "            y_train_fold = y[train_idx]\n",
    "            X_val_fold = X[val_idx]\n",
    "            y_val_fold = y[val_idx]\n",
    "            \n",
    "            # Build model for this fold\n",
    "            model = model_builder(trial, input_shape)\n",
    "            \n",
    "            # Enhanced callbacks\n",
    "            callbacks = [\n",
    "                EarlyStopping(\n",
    "                    monitor='val_loss', \n",
    "                    patience=patience, \n",
    "                    restore_best_weights=True, \n",
    "                    verbose=0\n",
    "                ),\n",
    "                ReduceLROnPlateau(\n",
    "                    monitor='val_loss', \n",
    "                    factor=0.8, \n",
    "                    patience=patience//3, \n",
    "                    min_lr=1e-8, \n",
    "                    verbose=0\n",
    "                ),\n",
    "                TFKerasPruningCallback(trial, 'val_loss')\n",
    "            ]\n",
    "            \n",
    "            # Train model\n",
    "            history = model.fit(\n",
    "                X_train_fold, y_train_fold,\n",
    "                validation_data=(X_val_fold, y_val_fold),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Get best validation loss\n",
    "            best_val_loss = min(history.history['val_loss'])\n",
    "            fold_scores.append(best_val_loss)\n",
    "            \n",
    "            # Clear memory\n",
    "            tf.keras.backend.clear_session()\n",
    "            del model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fold {fold_idx} failed: {e}\")\n",
    "            tf.keras.backend.clear_session()\n",
    "            fold_scores.append(float('inf'))\n",
    "    \n",
    "    # Return mean CV score with penalty for failed folds\n",
    "    valid_scores = [s for s in fold_scores if s != float('inf')]\n",
    "    if len(valid_scores) >= len(fold_scores) // 2:  # At least half successful\n",
    "        return np.mean(valid_scores)\n",
    "    else:\n",
    "        return float('inf')\n",
    "\n",
    "def optimize_enhanced_gru_architecture(architecture_name, X, y, cv_splits, n_trials=200):\n",
    "    \"\"\"Enhanced GRU optimization with more trials and better pruning\"\"\"\n",
    "    print(f\"\\nOptimizing {architecture_name} with enhanced methodology...\")\n",
    "    \n",
    "    # Create enhanced study\n",
    "    study_name = f\"enhanced_gru_{architecture_name.lower()}_optimization\"\n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        study_name=study_name,\n",
    "        pruner=optuna.pruners.MedianPruner(\n",
    "            n_startup_trials=10, \n",
    "            n_warmup_steps=15,\n",
    "            interval_steps=5\n",
    "        ),\n",
    "        sampler=optuna.samplers.TPESampler(\n",
    "            n_startup_trials=20,\n",
    "            n_ei_candidates=50\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Optimize with timeout\n",
    "    input_shape = (X.shape[1], X.shape[2])\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: enhanced_gru_cv_objective(trial, architecture_name, input_shape, X, y, cv_splits),\n",
    "        n_trials=n_trials,\n",
    "        timeout=7200  # 2 hour timeout\n",
    "    )\n",
    "    \n",
    "    return study\n",
    "\n",
    "print(\"Enhanced GRU optimization framework ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 5: Execute Enhanced GRU Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced optimization configuration\n",
    "gru_optimization_results = {}\n",
    "n_trials_per_architecture = 200  # Increased for better search\n",
    "\n",
    "print(\"Starting enhanced hyperparameter optimization for GRU architectures...\")\n",
    "print(f\"Trials per architecture: {n_trials_per_architecture}\")\n",
    "print(f\"Cross-validation folds: {len(cv_splits)}\")\n",
    "print(f\"Total estimated time: {len(ENHANCED_GRU_ARCHITECTURES) * n_trials_per_architecture * 4 / 60:.1f} minutes\")\n",
    "\n",
    "for arch_name in ENHANCED_GRU_ARCHITECTURES.keys():\n",
    "    print(f\"\\nOptimizing {arch_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Run enhanced optimization\n",
    "        study = optimize_enhanced_gru_architecture(\n",
    "            arch_name, \n",
    "            X_train, \n",
    "            y_train, \n",
    "            cv_splits, \n",
    "            n_trials=n_trials_per_architecture\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        gru_optimization_results[arch_name] = {\n",
    "            'study': study,\n",
    "            'best_params': study.best_params,\n",
    "            'best_value': study.best_value,\n",
    "            'n_trials': len(study.trials),\n",
    "            'n_complete_trials': len([t for t in study.trials if t.state == TrialState.COMPLETE])\n",
    "        }\n",
    "        \n",
    "        print(f\"Best CV loss for {arch_name}: {study.best_value:.6f}\")\n",
    "        print(f\"Completed trials: {gru_optimization_results[arch_name]['n_complete_trials']}/{len(study.trials)}\")\n",
    "        print(f\"Best parameters: {study.best_params}\")\n",
    "        \n",
    "        # Plot optimization history\n",
    "        fig = optuna.visualization.plot_optimization_history(study)\n",
    "        fig.show()\n",
    "        \n",
    "        # Plot parameter importance\n",
    "        if len(study.trials) > 10:\n",
    "            fig = optuna.visualization.plot_param_importances(study)\n",
    "            fig.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Optimization failed for {arch_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nEnhanced GRU hyperparameter optimization completed for {len(gru_optimization_results)} architectures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 6: Statistical Analysis and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_optimized_gru_model(architecture_name, best_params, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Evaluate optimized GRU model with statistical analysis\"\"\"\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    # Create mock trial with best parameters\n",
    "    class MockTrial:\n",
    "        def __init__(self, params):\n",
    "            self.params = params\n",
    "        \n",
    "        def suggest_int(self, name, low, high, step=1):\n",
    "            return self.params.get(name, (low + high) // 2)\n",
    "        \n",
    "        def suggest_float(self, name, low, high, log=False, step=None):\n",
    "            return self.params.get(name, (low + high) / 2)\n",
    "        \n",
    "        def suggest_categorical(self, name, choices):\n",
    "            return self.params.get(name, choices[0])\n",
    "    \n",
    "    mock_trial = MockTrial(best_params)\n",
    "    \n",
    "    # Build and train best model\n",
    "    model_builder = ENHANCED_GRU_ARCHITECTURES[architecture_name]\n",
    "    model = model_builder(mock_trial, input_shape)\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=0),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=8, min_lr=1e-8, verbose=0)\n",
    "    ]\n",
    "    \n",
    "    # Train on full training set\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=best_params.get('epochs', 100),\n",
    "        batch_size=best_params.get('batch_size', 32),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test, verbose=0).flatten()\n",
    "    \n",
    "    # Calculate comprehensive metrics\n",
    "    metrics = calculate_metrics(y_test, y_pred)\n",
    "    \n",
    "    # Calculate confidence intervals\n",
    "    lower_bound, upper_bound = calculate_confidence_intervals(y_pred)\n",
    "    \n",
    "    # Create baseline models for comparison\n",
    "    baselines = create_baseline_models(y_train, y_test)\n",
    "    \n",
    "    # Statistical significance tests against baselines\n",
    "    significance_tests = {}\n",
    "    model_errors = (y_test - y_pred) ** 2\n",
    "    \n",
    "    for baseline_name, baseline_pred in baselines.items():\n",
    "        baseline_errors = (y_test - baseline_pred) ** 2\n",
    "        dm_stat, p_value = diebold_mariano_test(model_errors, baseline_errors)\n",
    "        significance_tests[baseline_name] = {\n",
    "            'dm_statistic': dm_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05\n",
    "        }\n",
    "    \n",
    "    results = {\n",
    "        'architecture': architecture_name,\n",
    "        'best_params': best_params,\n",
    "        'metrics': metrics,\n",
    "        'training_time': training_time,\n",
    "        'total_params': model.count_params(),\n",
    "        'history': history.history,\n",
    "        'predictions': y_pred,\n",
    "        'confidence_intervals': {\n",
    "            'lower': lower_bound,\n",
    "            'upper': upper_bound\n",
    "        },\n",
    "        'baseline_comparisons': baselines,\n",
    "        'significance_tests': significance_tests\n",
    "    }\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    return results\n",
    "\n",
    "# Evaluate all optimized GRU models\n",
    "gru_evaluation_results = {}\n",
    "\n",
    "print(\"\\n=== EVALUATING OPTIMIZED GRU MODELS ===\")\n",
    "for arch_name, opt_result in gru_optimization_results.items():\n",
    "    print(f\"\\nEvaluating {arch_name}...\")\n",
    "    \n",
    "    try:\n",
    "        eval_result = evaluate_optimized_gru_model(\n",
    "            arch_name,\n",
    "            opt_result['best_params'],\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test\n",
    "        )\n",
    "        gru_evaluation_results[arch_name] = eval_result\n",
    "        \n",
    "        # Print results\n",
    "        metrics = eval_result['metrics']\n",
    "        print(f\"  MSE: {metrics['MSE']:.6f}\")\n",
    "        print(f\"  MAE: {metrics['MAE']:.6f}\")\n",
    "        print(f\"  RMSE: {metrics['RMSE']:.6f}\")\n",
    "        print(f\"  R²: {metrics['R2']:.6f}\")\n",
    "        print(f\"  Directional Accuracy: {metrics['Directional_Accuracy']:.4f}\")\n",
    "        print(f\"  Training Time: {eval_result['training_time']:.2f} seconds\")\n",
    "        print(f\"  Model Parameters: {eval_result['total_params']:,}\")\n",
    "        \n",
    "        # Print significance tests\n",
    "        print(f\"  Statistical Significance vs Baselines:\")\n",
    "        for baseline, test in eval_result['significance_tests'].items():\n",
    "            significance = \"✓\" if test['significant'] else \"✗\"\n",
    "            print(f\"    vs {baseline}: {significance} (p={test['p_value']:.4f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed for {arch_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n=== GRU OPTIMIZATION AND EVALUATION COMPLETED ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
