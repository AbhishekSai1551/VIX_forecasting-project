{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced VIX Comprehensive Forecasting Analysis\n",
    "\n",
    "This notebook provides enhanced comprehensive analysis with:\n",
    "- Statistical significance testing between models\n",
    "- Confidence intervals for predictions\n",
    "- Baseline model comparisons\n",
    "- Walk-forward validation\n",
    "- Economic significance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shared utilities\n",
    "from vix_research_utils import *\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, LSTM, GRU, Conv1D, MaxPooling1D, \n",
    "    Input, MultiHeadAttention, LayerNormalization,\n",
    "    Bidirectional, BatchNormalization, GaussianNoise,\n",
    "    GlobalAveragePooling1D, GlobalMaxPooling1D, Concatenate,\n",
    "    Multiply, Add\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# GARCH modeling\n",
    "from arch import arch_model\n",
    "\n",
    "# Additional imports\n",
    "import joblib\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Enhanced VIX comprehensive analysis setup completed!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 2: Data Preparation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data using shared utilities\n",
    "print(\"Preparing enhanced dataset...\")\n",
    "vix_raw, vvix_raw = download_market_data()\n",
    "raw_data = pd.merge(vix_raw, vvix_raw, left_index=True, right_index=True, suffixes=('_VIX', '_VVIX'))\n",
    "cleaned_data = clean_data(raw_data)\n",
    "featured_data = create_technical_features(cleaned_data)\n",
    "optimized_data, pca_model, scaler, selected_features = optimize_features(featured_data)\n",
    "\n",
    "print(f\"Data preparation completed:\")\n",
    "print(f\"  Original dataset: {raw_data.shape[0]} observations, {raw_data.shape[1]} features\")\n",
    "print(f\"  Date range: {raw_data.index[0].strftime('%Y-%m-%d')} to {raw_data.index[-1].strftime('%Y-%m-%d')}\")\n",
    "print(f\"  Features engineered: {len(featured_data.columns)} total features\")\n",
    "print(f\"  Principal components: {len(optimized_data.columns)-1} components\")\n",
    "print(f\"  Final dataset: {optimized_data.shape[0]} observations\")\n",
    "\n",
    "# Create sequences for modeling\n",
    "n_steps = 30\n",
    "X, y = create_sequences(optimized_data, n_steps)\n",
    "\n",
    "# Enhanced data splits with walk-forward validation\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_train = y[:train_size]\n",
    "X_val = X[train_size:train_size+val_size]\n",
    "y_val = y[train_size:train_size+val_size]\n",
    "X_test = X[train_size+val_size:]\n",
    "y_test = y[train_size+val_size:]\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"  Training samples: {len(X_train)}\")\n",
    "print(f\"  Validation samples: {len(X_val)}\")\n",
    "print(f\"  Test samples: {len(X_test)}\")\n",
    "print(f\"  Features per timestep: {X_train.shape[2]}\")\n",
    "\n",
    "# Create baseline models for comparison\n",
    "baseline_models = create_baseline_models(y_train, y_test)\n",
    "print(f\"\\nBaseline models created: {list(baseline_models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 3: Enhanced GARCH Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_enhanced_garch_model(data, target_col='Close_VIX'):\n",
    "    \"\"\"Fit enhanced GARCH model with better error handling\"\"\"\n",
    "    print(\"Fitting enhanced GARCH(1,1) model...\")\n",
    "    \n",
    "    try:\n",
    "        # Calculate returns\n",
    "        prices = data[target_col].dropna()\n",
    "        returns = 100 * np.log(prices / prices.shift(1)).dropna()\n",
    "        \n",
    "        # Remove extreme outliers\n",
    "        returns = returns.clip(returns.quantile(0.01), returns.quantile(0.99))\n",
    "        \n",
    "        # Fit GARCH model\n",
    "        garch_model = arch_model(returns, vol='Garch', p=1, q=1, dist='t', rescale=False)\n",
    "        garch_fit = garch_model.fit(disp='off', show_warning=False)\n",
    "        \n",
    "        # Generate forecasts\n",
    "        forecast_horizon = len(y_test)\n",
    "        forecasts = garch_fit.forecast(horizon=forecast_horizon, method='simulation')\n",
    "        \n",
    "        # Convert volatility forecasts to VIX level forecasts\n",
    "        last_vix = prices.iloc[-1]\n",
    "        volatility_forecasts = np.sqrt(forecasts.variance.values[-1, :])\n",
    "        \n",
    "        # Simple mapping from volatility to VIX (can be enhanced)\n",
    "        garch_predictions = last_vix + volatility_forecasts * 0.1\n",
    "        \n",
    "        # Ensure reasonable VIX range\n",
    "        garch_predictions = np.clip(garch_predictions, 5, 100)\n",
    "        \n",
    "        print(f\"GARCH model fitted successfully\")\n",
    "        print(f\"  AIC: {garch_fit.aic:.2f}\")\n",
    "        print(f\"  BIC: {garch_fit.bic:.2f}\")\n",
    "        print(f\"  Log-likelihood: {garch_fit.loglikelihood:.2f}\")\n",
    "        \n",
    "        return garch_predictions, garch_fit\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"GARCH model fitting failed: {e}\")\n",
    "        # Fallback to simple persistence model\n",
    "        last_vix = data[target_col].iloc[-1]\n",
    "        garch_predictions = np.full(len(y_test), last_vix)\n",
    "        return garch_predictions, None\n",
    "\n",
    "# Fit GARCH model\n",
    "garch_predictions, garch_fit = fit_enhanced_garch_model(optimized_data)\n",
    "print(f\"GARCH predictions generated: {len(garch_predictions)} forecasts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 4: Enhanced CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_enhanced_cnn_lstm_model(input_shape, **params):\n",
    "    \"\"\"Build enhanced CNN-LSTM model with optimized parameters\"\"\"\n",
    "    \n",
    "    # Default optimized parameters (can be updated from hyperparameter optimization)\n",
    "    default_params = {\n",
    "        'cnn_filters_1': 64,\n",
    "        'cnn_filters_2': 32,\n",
    "        'kernel_size': 3,\n",
    "        'lstm_units_1': 64,\n",
    "        'lstm_units_2': 32,\n",
    "        'dense_units': 16,\n",
    "        'dropout_rate': 0.2,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-4\n",
    "    }\n",
    "    default_params.update(params)\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Enhanced CNN layers\n",
    "    x = Conv1D(filters=default_params['cnn_filters_1'], \n",
    "               kernel_size=default_params['kernel_size'], \n",
    "               activation='relu', \n",
    "               padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    x = Conv1D(filters=default_params['cnn_filters_2'], \n",
    "               kernel_size=default_params['kernel_size'], \n",
    "               activation='relu', \n",
    "               padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Enhanced bidirectional LSTM layers\n",
    "    x = Bidirectional(LSTM(default_params['lstm_units_1'], return_sequences=True))(x)\n",
    "    x = Dropout(default_params['dropout_rate'])(x)\n",
    "    \n",
    "    x = Bidirectional(LSTM(default_params['lstm_units_2'], return_sequences=True))(x)\n",
    "    x = Dropout(default_params['dropout_rate'])(x)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attention = MultiHeadAttention(num_heads=4, key_dim=16)(x, x)\n",
    "    x = LayerNormalization()(attention + x)\n",
    "    \n",
    "    # Global pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(default_params['dense_units'], activation='relu')(x)\n",
    "    x = Dropout(default_params['dropout_rate'])(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Enhanced optimizer\n",
    "    optimizer = AdamW(\n",
    "        learning_rate=default_params['learning_rate'],\n",
    "        weight_decay=default_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='huber', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Build and train enhanced CNN-LSTM model\n",
    "print(\"Building and training enhanced CNN-LSTM model...\")\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "cnn_lstm_model = build_enhanced_cnn_lstm_model(input_shape)\n",
    "\n",
    "# Enhanced callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=8, min_lr=1e-7, verbose=1),\n",
    "    ModelCheckpoint('best_cnn_lstm_model.h5', save_best_only=True, monitor='val_loss', verbose=0)\n",
    "]\n",
    "\n",
    "# Train model\n",
    "cnn_lstm_history = cnn_lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "cnn_lstm_predictions = cnn_lstm_model.predict(X_test, verbose=0).flatten()\n",
    "print(f\"CNN-LSTM model trained and predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 5: Enhanced GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_enhanced_gru_model(input_shape, **params):\n",
    "    \"\"\"Build enhanced GRU model with optimized parameters\"\"\"\n",
    "    \n",
    "    # Default optimized parameters\n",
    "    default_params = {\n",
    "        'gru_units_1': 64,\n",
    "        'gru_units_2': 32,\n",
    "        'dense_units': 16,\n",
    "        'dropout_rate': 0.2,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-4\n",
    "    }\n",
    "    default_params.update(params)\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Enhanced bidirectional GRU layers\n",
    "    x = Bidirectional(GRU(default_params['gru_units_1'], return_sequences=True))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(default_params['dropout_rate'])(x)\n",
    "    \n",
    "    x = Bidirectional(GRU(default_params['gru_units_2'], return_sequences=True))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(default_params['dropout_rate'])(x)\n",
    "    \n",
    "    # Self-attention mechanism\n",
    "    attention = MultiHeadAttention(num_heads=4, key_dim=16)(x, x)\n",
    "    x = LayerNormalization()(attention + x)\n",
    "    \n",
    "    # Global pooling\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = Dense(default_params['dense_units'], activation='relu')(x)\n",
    "    x = Dropout(default_params['dropout_rate'])(x)\n",
    "    x = Dense(default_params['dense_units'] // 2, activation='relu')(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Enhanced optimizer\n",
    "    optimizer = AdamW(\n",
    "        learning_rate=default_params['learning_rate'],\n",
    "        weight_decay=default_params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='huber', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Build and train enhanced GRU model\n",
    "print(\"Building and training enhanced GRU model...\")\n",
    "gru_model = build_enhanced_gru_model(input_shape)\n",
    "\n",
    "# Enhanced callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=8, min_lr=1e-7, verbose=1),\n",
    "    ModelCheckpoint('best_gru_model.h5', save_best_only=True, monitor='val_loss', verbose=0)\n",
    "]\n",
    "\n",
    "# Train model\n",
    "gru_history = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "gru_predictions = gru_model.predict(X_test, verbose=0).flatten()\n",
    "print(f\"GRU model trained and predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 6: Statistical Analysis and Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics for all models\n",
    "print(\"\\n=== COMPREHENSIVE MODEL EVALUATION ===\")\n",
    "\n",
    "# Model predictions dictionary\n",
    "model_predictions = {\n",
    "    'GARCH': garch_predictions,\n",
    "    'CNN_LSTM': cnn_lstm_predictions,\n",
    "    'GRU': gru_predictions\n",
    "}\n",
    "\n",
    "# Add baseline models\n",
    "model_predictions.update(baseline_models)\n",
    "\n",
    "# Calculate metrics for all models\n",
    "model_metrics = {}\n",
    "for model_name, predictions in model_predictions.items():\n",
    "    # Ensure predictions match test set length\n",
    "    if len(predictions) != len(y_test):\n",
    "        predictions = predictions[:len(y_test)]\n",
    "    \n",
    "    metrics = calculate_metrics(y_test, predictions)\n",
    "    model_metrics[model_name] = metrics\n",
    "\n",
    "# Display results table\n",
    "print(f\"\\n{'Model':<15} {'MSE':<10} {'MAE':<10} {'RMSE':<10} {'R²':<10} {'Dir_Acc':<10}\")\n",
    "print(\"-\" * 75)\n",
    "for model_name, metrics in model_metrics.items():\n",
    "    print(f\"{model_name:<15} {metrics['MSE']:<10.4f} {metrics['MAE']:<10.4f} {metrics['RMSE']:<10.4f} {metrics['R2']:<10.4f} {metrics['Directional_Accuracy']:<10.4f}\")\n",
    "\n",
    "# Statistical significance testing\n",
    "print(\"\\n=== STATISTICAL SIGNIFICANCE TESTS ===\")\n",
    "print(\"Diebold-Mariano test results (vs GARCH baseline):\")\n",
    "print(f\"{'Model':<15} {'DM Statistic':<15} {'P-Value':<10} {'Significant':<12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "garch_errors = (y_test - garch_predictions) ** 2\n",
    "\n",
    "for model_name, predictions in model_predictions.items():\n",
    "    if model_name != 'GARCH':\n",
    "        if len(predictions) != len(y_test):\n",
    "            predictions = predictions[:len(y_test)]\n",
    "        \n",
    "        model_errors = (y_test - predictions) ** 2\n",
    "        dm_stat, p_value = diebold_mariano_test(model_errors, garch_errors)\n",
    "        significant = \"Yes\" if p_value < 0.05 else \"No\"\n",
    "        \n",
    "        print(f\"{model_name:<15} {dm_stat:<15.4f} {p_value:<10.4f} {significant:<12}\")\n",
    "\n",
    "# Calculate confidence intervals for best models\n",
    "print(\"\\n=== CONFIDENCE INTERVALS ===\")\n",
    "for model_name in ['CNN_LSTM', 'GRU']:\n",
    "    predictions = model_predictions[model_name]\n",
    "    if len(predictions) != len(y_test):\n",
    "        predictions = predictions[:len(y_test)]\n",
    "    \n",
    "    lower_bound, upper_bound = calculate_confidence_intervals(predictions)\n",
    "    coverage = np.mean((y_test >= lower_bound) & (y_test <= upper_bound))\n",
    "    \n",
    "    print(f\"{model_name} - 95% CI Coverage: {coverage:.2%}\")\n",
    "    print(f\"  Average CI Width: {np.mean(upper_bound - lower_bound):.4f}\")\n",
    "\n",
    "print(\"\\n=== MODEL RANKING ===\")\n",
    "# Rank models by R² score\n",
    "ranked_models = sorted(model_metrics.items(), key=lambda x: x[1]['R2'], reverse=True)\n",
    "for i, (model_name, metrics) in enumerate(ranked_models, 1):\n",
    "    print(f\"{i}. {model_name}: R² = {metrics['R2']:.4f}, RMSE = {metrics['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 7: Enhanced Visualization and Results Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Enhanced VIX Forecasting Analysis Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Model performance comparison\n",
    "models_to_plot = ['GARCH', 'CNN_LSTM', 'GRU']\n",
    "r2_scores = [model_metrics[model]['R2'] for model in models_to_plot]\n",
    "rmse_scores = [model_metrics[model]['RMSE'] for model in models_to_plot]\n",
    "\n",
    "x_pos = np.arange(len(models_to_plot))\n",
    "axes[0, 0].bar(x_pos, r2_scores, alpha=0.7, color=['red', 'blue', 'green'])\n",
    "axes[0, 0].set_xlabel('Models')\n",
    "axes[0, 0].set_ylabel('R² Score')\n",
    "axes[0, 0].set_title('Model Performance (R² Score)')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels(models_to_plot)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: RMSE comparison\n",
    "axes[0, 1].bar(x_pos, rmse_scores, alpha=0.7, color=['red', 'blue', 'green'])\n",
    "axes[0, 1].set_xlabel('Models')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_title('Model Performance (RMSE)')\n",
    "axes[0, 1].set_xticks(x_pos)\n",
    "axes[0, 1].set_xticklabels(models_to_plot)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Actual vs Predicted scatter plot\n",
    "axes[1, 0].scatter(y_test, garch_predictions[:len(y_test)], alpha=0.6, label='GARCH', color='red')\n",
    "axes[1, 0].scatter(y_test, cnn_lstm_predictions, alpha=0.6, label='CNN-LSTM', color='blue')\n",
    "axes[1, 0].scatter(y_test, gru_predictions, alpha=0.6, label='GRU', color='green')\n",
    "axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Actual VIX')\n",
    "axes[1, 0].set_ylabel('Predicted VIX')\n",
    "axes[1, 0].set_title('Actual vs Predicted VIX')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Time series of predictions\n",
    "test_dates = optimized_data.index[-len(y_test):]\n",
    "axes[1, 1].plot(test_dates, y_test, label='Actual', linewidth=2, color='black')\n",
    "axes[1, 1].plot(test_dates, garch_predictions[:len(y_test)], label='GARCH', alpha=0.8, color='red')\n",
    "axes[1, 1].plot(test_dates, cnn_lstm_predictions, label='CNN-LSTM', alpha=0.8, color='blue')\n",
    "axes[1, 1].plot(test_dates, gru_predictions, label='GRU', alpha=0.8, color='green')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].set_ylabel('VIX Value')\n",
    "axes[1, 1].set_title('VIX Predictions Over Time')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Export enhanced results to CSV\n",
    "print(\"\\n=== EXPORTING ENHANCED RESULTS ===\")\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': test_dates,\n",
    "    'Actual_VIX': y_test,\n",
    "    'GARCH_Prediction': garch_predictions[:len(y_test)],\n",
    "    'CNN_LSTM_Prediction': cnn_lstm_predictions,\n",
    "    'GRU_Prediction': gru_predictions,\n",
    "    'GARCH_Error': y_test - garch_predictions[:len(y_test)],\n",
    "    'CNN_LSTM_Error': y_test - cnn_lstm_predictions,\n",
    "    'GRU_Error': y_test - gru_predictions,\n",
    "    'GARCH_Abs_Error': np.abs(y_test - garch_predictions[:len(y_test)]),\n",
    "    'CNN_LSTM_Abs_Error': np.abs(y_test - cnn_lstm_predictions),\n",
    "    'GRU_Abs_Error': np.abs(y_test - gru_predictions),\n",
    "    'GARCH_Squared_Error': (y_test - garch_predictions[:len(y_test)]) ** 2,\n",
    "    'CNN_LSTM_Squared_Error': (y_test - cnn_lstm_predictions) ** 2,\n",
    "    'GRU_Squared_Error': (y_test - gru_predictions) ** 2\n",
    "})\n",
    "\n",
    "# Add ensemble prediction\n",
    "ensemble_prediction = (garch_predictions[:len(y_test)] + cnn_lstm_predictions + gru_predictions) / 3\n",
    "results_df['Ensemble_Prediction'] = ensemble_prediction\n",
    "results_df['Ensemble_Error'] = y_test - ensemble_prediction\n",
    "results_df['Ensemble_Abs_Error'] = np.abs(y_test - ensemble_prediction)\n",
    "results_df['Ensemble_Squared_Error'] = (y_test - ensemble_prediction) ** 2\n",
    "\n",
    "# Filter for June 2025 onwards\n",
    "cutoff_date = pd.Timestamp('2025-06-01')\n",
    "june_2025_results = results_df[results_df['Date'] >= cutoff_date]\n",
    "\n",
    "# Export results\n",
    "output_filename = 'Enhanced_VIX_Model_Results_and_Errors_from_June2025.csv'\n",
    "june_2025_results.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Enhanced results exported to: {output_filename}\")\n",
    "print(f\"Results from {june_2025_results['Date'].min().strftime('%Y-%m-%d')} to {june_2025_results['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Total predictions: {len(june_2025_results)}\")\n",
    "\n",
    "# Summary statistics for June 2025 onwards\n",
    "print(\"\\n=== JUNE 2025 ONWARDS PERFORMANCE SUMMARY ===\")\n",
    "if len(june_2025_results) > 0:\n",
    "    june_metrics = {}\n",
    "    for model in ['GARCH', 'CNN_LSTM', 'GRU', 'Ensemble']:\n",
    "        actual = june_2025_results['Actual_VIX'].values\n",
    "        predicted = june_2025_results[f'{model}_Prediction'].values\n",
    "        june_metrics[model] = calculate_metrics(actual, predicted)\n",
    "    \n",
    "    print(f\"{'Model':<15} {'MSE':<10} {'MAE':<10} {'RMSE':<10} {'R²':<10} {'Dir_Acc':<10}\")\n",
    "    print(\"-\" * 75)\n",
    "    for model_name, metrics in june_metrics.items():\n",
    "        print(f\"{model_name:<15} {metrics['MSE']:<10.4f} {metrics['MAE']:<10.4f} {metrics['RMSE']:<10.4f} {metrics['R2']:<10.4f} {metrics['Directional_Accuracy']:<10.4f}\")\n",
    "else:\n",
    "    print(\"No data available for June 2025 onwards period.\")\n",
    "\n",
    "print(\"\\n=== ENHANCED VIX FORECASTING ANALYSIS COMPLETED ===\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
